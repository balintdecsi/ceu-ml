{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd06167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set options for better readability of numbers\n",
    "%precision 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33355e3e",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "PCA is to transform the data X into another space such that new features are uncorrelated and preserve the variance of the original data.\n",
    "Simply put, we reduce dimensions by eliminating useless (or least useful) features.\n",
    "\n",
    "What if they are correlated? Keeping one or the other may lead to misleading conclusions.\n",
    "PCA is a way to reduce dimensions while preserving the variation in the data.\n",
    "\n",
    "\n",
    "For a very nice illustration and discussion, see [this Cross Validated post](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb6d95",
   "metadata": {},
   "source": [
    "## Illustration of PCA on a simple simulated dataset with 2 correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d9ce7",
   "metadata": {},
   "source": [
    "### Technical detour: best practice for pseudo random number generation and ensuring reproducibility\n",
    "\n",
    "Ensuring reproducibility by setting the global seed with `np.random.seed()` is a risky practice as it modifies the global state. Some imported packages and functions may be unintentionally affected.\n",
    "\n",
    "The recommended practice instead is to create a new (psuedo) random number generator and pass it around.\n",
    "Call `np.random.RandomState(<seed>)` to create a new RNG. This can be either used to create random numbers directly, or added as an argument to a function that requires a random state (e.g. sklearn functions).\n",
    "\n",
    "See more details [here](https://scikit-learn.org/stable/common_pitfalls.html#controlling-randomness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "094bcc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prng = np.random.RandomState(20250303)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f74ab7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [HELPER FUNCTIONS] Generate correlated features\n",
    "def generateRandomCovarianceVarianceMatrix(n_features, prng=None):\n",
    "    if prng is None:\n",
    "        prng = np.random.RandomState()\n",
    "\n",
    "    random_matrix = prng.rand(n_features, n_features)\n",
    "    \n",
    "    # Construct a symmetric positive definite matrix\n",
    "    var_cov_matrix = np.dot(random_matrix, random_matrix.T)\n",
    "    \n",
    "    # Normalize diagonal to ensure reasonable variances\n",
    "    diag = np.sqrt(np.diag(var_cov_matrix))\n",
    "    var_cov_matrix = var_cov_matrix / np.outer(diag, diag)  # Correlation-like structure\n",
    "\n",
    "    # Ensure unit variance by dividing by diagonal elements\n",
    "    var_cov_matrix = var_cov_matrix / np.diag(var_cov_matrix)[:, np.newaxis]\n",
    "    \n",
    "    return var_cov_matrix\n",
    "    \n",
    "def generateCorrelatedFeatures(n_features, n_samples, vc_matrix=None, prng=None):\n",
    "    if prng is None:\n",
    "        prng = np.random.RandomState()\n",
    "\n",
    "    if vc_matrix is None:\n",
    "        vc_matrix = generateRandomCovarianceVarianceMatrix(n_features, prng)\n",
    "        \n",
    "    data = prng.multivariate_normal(np.repeat(0, n_features), vc_matrix, size=n_samples)\n",
    "    return pd.DataFrame(data, columns=[f'Feature {i+1}' for i in range(n_features)]) # work with Pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd28a0d5",
   "metadata": {},
   "source": [
    "### Data generation (2 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b51b2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "n_samples = 100\n",
    "n_features = 2\n",
    "X = generateCorrelatedFeatures(n_features, n_samples, prng=prng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17fd4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(X, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original data\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.scatter(X[\"Feature 1\"], X[\"Feature 2\"], alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Data Points\")\n",
    "\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e9e824",
   "metadata": {},
   "source": [
    "### Motivating PCA: projection of data onto a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9ac2da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [HELPER FUNCTIONS] Create plot with points projected to a certain direction\n",
    "def plotDataWithProjection(direction_vector, X):\n",
    "    direction = direction_vector / np.linalg.norm(direction_vector)  # normalize to unit vector\n",
    "    \n",
    "    # Plot original data points\n",
    "    plt.scatter(X[\"Feature 1\"], X[\"Feature 2\"], alpha=0.5, label='Original points')\n",
    "\n",
    "    # Draw the direction vector as a line across the chart\n",
    "    plt.plot(\n",
    "        [-5 * direction[0], 5 * direction[0]], [-5 * direction[1], 5 * direction[1]], \n",
    "        color='darkred', alpha=0.5, label='Projection direction'\n",
    "    )\n",
    "\n",
    "    # Calculate projections\n",
    "    X_array = X.values\n",
    "    projections = np.outer(X_array @ direction, direction)\n",
    "\n",
    "    # Plot projection points and lines\n",
    "    plt.scatter(projections[:, 0], projections[:, 1], color='darkred', alpha=0.5, label='Projected points')\n",
    "\n",
    "    # Draw lines between original points and their projections\n",
    "    for i in range(len(X)):\n",
    "        plt.plot([X_array[i, 0], projections[i, 0]], [X_array[i, 1], projections[i, 1]], color='darkred', alpha=0.2)\n",
    "\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('Data Points with Projections')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2x2 grid of plots with different direction vectors\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(16, 16))\n",
    "\n",
    "# Store current axis to restore later\n",
    "current_ax = plt.gca()\n",
    "\n",
    "# Plot with different direction vectors\n",
    "plt.sca(ax1)\n",
    "plotDataWithProjection([1, 0], X)\n",
    "ax1.set_title(\"Direction [1, 0]\")\n",
    "\n",
    "plt.sca(ax2) \n",
    "plotDataWithProjection([0, 1], X)\n",
    "ax2.set_title(\"Direction [0, 1]\")\n",
    "\n",
    "plt.sca(ax3)\n",
    "plotDataWithProjection([0.5, 0.5], X)\n",
    "ax3.set_title(\"Direction [0.5, 0.5]\")\n",
    "\n",
    "plt.sca(ax4)\n",
    "plotDataWithProjection([-0.5, 0.5], X)\n",
    "ax4.set_title(\"Direction [-0.5, 0.5]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Restore the axis for subsequent plots\n",
    "plt.sca(current_ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9654f38",
   "metadata": {},
   "source": [
    "### Running PCA on a simple example of 2 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd94fab",
   "metadata": {},
   "source": [
    "#### Technical detour: working of sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0daf987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA and calculate the principal components as X_pca\n",
    "pca = PCA() #definition of the model\n",
    "pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b914975",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "def52f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.transform(X)\n",
    "\n",
    "# You can also use fit_transform() to fit the model and transform the data at the same time\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17e0c09",
   "metadata": {},
   "source": [
    "Note that the result is a numpy array, not a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1482ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type of X: \", type(X))\n",
    "print(\"Type of X_pca: \", type(X_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4787b7",
   "metadata": {},
   "source": [
    "*Question:* Why donâ€™t we set a random state for the PCA model? Will the results be reproducible?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "    \n",
    "PCA is a deterministic algorithm, so the results are reproducible regardless of the random state.\n",
    "\n",
    "The only difference that could be observed is the difference in the sign of the principal components, which is not important.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a0252f",
   "metadata": {},
   "source": [
    "#### Check characteristics of PCA\n",
    "\n",
    "Recap: We transform the coordinates of the original variables to capture as much variation as we can with independent (orthogonal) dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08610ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the space of X to the space of principal components\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(16, 8))\n",
    "    \n",
    "fig.suptitle('Transform X into PC')\n",
    "\n",
    "ax1.scatter(X[\"Feature 1\"], X[\"Feature 2\"], alpha=0.5)\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.set_xlim(-4, 4)\n",
    "ax1.set_ylim(-4, 4)\n",
    "\n",
    "# add the principal component transformation vectors to the plot once you know where to get the loadings from\n",
    "ax1.arrow(0, 0, pca.components_[0, 0], pca.components_[1, 0], color='darkred', width=0.05)\n",
    "ax1.arrow(0, 0, pca.components_[0, 1], pca.components_[1, 1], color='darkred', width=0.05)\n",
    "\n",
    "# add a new plot about the transformed features\n",
    "ax2.scatter(X_pca[:, 0], X_pca[:, 1], color='darkred', alpha=0.5)\n",
    "ax2.set_xlabel('Principal Component 1')\n",
    "ax2.set_ylabel('Principal Component 2')\n",
    "ax2.set_xlim(-4, 4)\n",
    "ax2.set_ylim(-4, 4)\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e837961a",
   "metadata": {},
   "source": [
    "##### Explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7918d58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variance = np.var(X, axis=0).sum()\n",
    "total_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f1b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac98db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498e4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The first principal component explains {round(pca.explained_variance_ratio_[0] * 100, 1)}% of the total variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf90b77",
   "metadata": {},
   "source": [
    "##### Loadings/weights: how to transform the space of X into the space of principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2302221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1eeb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loadings\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(\n",
    "    pca.components_, \n",
    "    cmap='RdBu', center=0, annot=True, fmt='.3f', \n",
    "    xticklabels=['Feature 1', 'Feature 2'], yticklabels=['PC1', 'PC2']\n",
    ")\n",
    "plt.title('PCA Components Loadings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de9fe5",
   "metadata": {},
   "source": [
    "##### Orthogonality of principal components\n",
    "\n",
    "Each principal component contains \"independent\" variance from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d59687",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pca.components_[:, 0] * pca.components_[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea32b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plotDataWithProjection(pca.components_[0], X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9103a4",
   "metadata": {},
   "source": [
    "##### Reconstruction error using only the first principal component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d0d9647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1 = PCA(n_components=1)\n",
    "X_pca1 = pca1.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77067bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double-check that PC1 of the original PCA modell is the same\n",
    "for i in range(5):\n",
    "    print(X_pca[i, 0], X_pca1[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3880a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_data = pca1.inverse_transform(X_pca1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original data and the reconstructed data\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.scatter(X[\"Feature 1\"], X[\"Feature 2\"], alpha=0.5, label=\"Original\")\n",
    "plt.scatter(reconstructed_data[:, 0], reconstructed_data[:, 1], alpha=0.5, color=\"darkred\", label=\"Reconstructed\")\n",
    "\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Data Points (original vs reconstructed)\")\n",
    "plt.legend()\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f6bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reconstruction error (average Euclidean distance)\n",
    "def calculateReconstructionError(X, X_reconstructed):\n",
    "    return np.mean(np.sum(np.square(X - X_reconstructed), axis=1))\n",
    "\n",
    "\n",
    "reconstruction_error = calculateReconstructionError(X, reconstructed_data)\n",
    "reconstruction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7727bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_error / total_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf1d50c",
   "metadata": {},
   "source": [
    "### Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f723cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = train_test_split(X, test_size=0.3, random_state=prng)\n",
    "\n",
    "# Fit PCA on training data\n",
    "pca_train = PCA(n_components=1)\n",
    "X_train_pca = pca_train.fit_transform(X_train)\n",
    "\n",
    "# Transform test data using PCA fit on training data\n",
    "X_test_pca = pca_train.transform(X_test)\n",
    "\n",
    "# Reconstruct test data\n",
    "X_test_reconstructed = pca_train.inverse_transform(X_test_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b1dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original dimension of the test data: \", X_test.shape)\n",
    "print(\"Transformed dimension of the test data (dimensionality reduction): \", X_test_pca.shape)\n",
    "print(\"Reconstructed dimension of the test data: \", X_test_reconstructed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df814b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reconstruction errors on test and train sets\n",
    "test_reconstruction_error = calculateReconstructionError(X_test, X_test_reconstructed)\n",
    "\n",
    "X_train_reconstructed = pca_train.inverse_transform(X_train_pca)\n",
    "train_reconstruction_error = calculateReconstructionError(X_train, X_train_reconstructed)\n",
    "\n",
    "print(f\"Train set reconstruction error: {train_reconstruction_error:.4f}\")\n",
    "print(f\"Test set reconstruction error: {test_reconstruction_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab4594",
   "metadata": {},
   "source": [
    "### Relation to OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "61aec081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "ols = LinearRegression().fit(X[[\"Feature 1\"]], X[\"Feature 2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5638c26b",
   "metadata": {},
   "source": [
    "#### Technical detour: Series vs DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c7ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Single brackets create a Series, not a DataFrame: {type(X[\"Feature 1\"])}\")\n",
    "print(f\"Double brackets create a DataFrame with one column: {type(X[[\"Feature 1\"]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d652d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original data, the first principal component direction, and the OLS fit\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Original data points\n",
    "plt.scatter(X[\"Feature 1\"], X[\"Feature 2\"], alpha=0.5, label=\"Original\")\n",
    "\n",
    "# Plot a line along the PC1 direction\n",
    "x_line = np.array([-3, 3])\n",
    "y_line = (pca1.components_[0][1] / pca1.components_[0][0]) * x_line\n",
    "plt.plot(x_line, y_line, color=\"darkred\", linewidth=2, label=\"PC1\")\n",
    "\n",
    "# OLS fit of Feature 2 on Feature 1\n",
    "y_ols = ols.intercept_ + ols.coef_ * x_line\n",
    "plt.plot(x_line, y_ols, '--', color=\"darkgreen\", linewidth=2, label=\"OLS Fit (Feature 2 ~ Feature 1)\")\n",
    "\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"PC1 vs OLS Fit\")\n",
    "plt.legend()\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f7cd1",
   "metadata": {},
   "source": [
    "## Running PCA on a larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4b44ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "n_samples = 100\n",
    "n_features = 50\n",
    "X_large = pd.DataFrame(prng.multivariate_normal(mean=np.repeat(0, n_features), cov=np.eye(n_features), size=n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7695ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is indeed large, hard to visualize\n",
    "X_large.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312c77ad",
   "metadata": {},
   "source": [
    "**TODO**: \n",
    "- create a PCA model\n",
    "- train with the large dataset\n",
    "- check the explained variance ratio for each component\n",
    "- How many components do we need to explain at least 90% of the total variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction error based on the first n_components_90 principal components\n",
    "pca_large_90 = PCA(n_components=n_components_90) # definition of the model\n",
    "pca_large_90.fit(X_large) # training of the model\n",
    "\n",
    "X_large_reconstructed_90 = pca_large_90.inverse_transform(pca_large_90.transform(X_large))\n",
    "\n",
    "# Calculate reconstruction error (RMSE)   \n",
    "rmse_large = calculateReconstructionError(X_large, X_large_reconstructed_90)\n",
    "rmse_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f079e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variance_large = np.var(X_large, axis=0).sum()\n",
    "total_variance_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_large / total_variance_large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cb94c5",
   "metadata": {},
   "source": [
    "### Generalization\n",
    "\n",
    "**TODO**:\n",
    "- split the data into train (70%) and test (30%) set\n",
    "- create a PCA model with the number of components we identified above\n",
    "- fit the model on the train data\n",
    "- calculate the reconstruction error for both the train and the test data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CEU-ML)",
   "language": "python",
   "name": "ceu-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
