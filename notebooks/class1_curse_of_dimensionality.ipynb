{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4871fce",
   "metadata": {},
   "source": [
    "# Class 1 — Similarity, High Dimensions, and Why PCA\n",
    "\n",
    "**Big idea:** most supervised learning methods rely (explicitly or implicitly) on: *similar items have similar outcomes*.\n",
    "\n",
    "Today we will:\n",
    "- Make the idea of **similarity** concrete (via distances)\n",
    "- See why similarity becomes tricky in **high dimensions** (curse of dimensionality)\n",
    "- Motivate **dimensionality reduction** and **PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 110\n",
    "%precision 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37365f05",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n",
    "\n",
    "In your previous supervised learning course, you trained multiple Machine Learning models. All of these models share an implicit assumption:\n",
    "**Similar** items have **similar** outcomes. \n",
    "\n",
    "For example:\n",
    "- Two apartments similar in location, size, and amenities should have similar prices\n",
    "- Used cars of similar age should have similar prices\n",
    "\n",
    "This seems intuitive, but as we'll see, defining \"similar\" becomes surprisingly tricky in high dimensions.\n",
    "\n",
    "To make *similar* mathematically precise, we use **distance** in feature space:\n",
    "\n",
    "- **Euclidean distance** (most common): straight-line distance, like measuring with a ruler\n",
    "- **Manhattan distance** (more robust): sum of absolute differences along each dimension, like walking city blocks in Manhattan\n",
    "- **Cosine distance** (for direction): measures angle between vectors, ranging from 0 (same direction) to 1 (orthogonal)\n",
    "\n",
    "**Connection to what you know:** Ordinary Least Squares (OLS) regression finds coefficients that minimize the Euclidean *distance* between observed outcomes $y$ and predicted outcomes $\\mathbf{X}'\\hat \\beta$.\n",
    "\n",
    "**The problem:** As we move into higher dimensions, distance becomes less reliable for expressing similarity. Let's see why.\n",
    "\n",
    "### Problem 1: Sparsity (everything lives at the edges)\n",
    "\n",
    "As dimensions increase, data becomes increasingly **sparse**: most points end up near the boundaries rather than in the interior of the space.\n",
    "\n",
    "**Simple example:** Consider uniformly distributed random points in a [0, 1] box. Define \"internal points\" as those not within 2% of any edge (i.e., in the range [0.02, 0.98]).\n",
    "\n",
    "- **1 dimension:** Probability of being internal = 0.96 (96%)\n",
    "- **2 dimensions:** Probability = 0.96² = 0.92 (92%)\n",
    "- **3 dimensions:** Probability = 0.96³ = 0.88 (88%)\n",
    "- **100 dimensions:** Probability = 0.96¹⁰⁰ ≈ 0.017 (less than 2%!)\n",
    "\n",
    "**Why this matters:** In high-dimensional spaces, almost all data points cluster near the boundaries. This is counterintuitive and makes statistical sampling, clustering, and distance-based algorithms much less effective.\n",
    "\n",
    "### Problem 2: Distance concentration (everything becomes equidistant)\n",
    "\n",
    "As dimensionality increases, the average distance between points obviously grows. But here's the counterintuitive part: the **ratio** between the nearest and farthest neighbor approaches 1.\n",
    "Thus, in very high dimensions, all points become almost equidistant from each other. Your \"closest\" neighbor isn't really that close anymore!\n",
    "\n",
    "**Why this matters:** This undermines the fundamental assumption behind many ML algorithms, that nearby points should be more similar than distant ones. If everything is roughly the same distance away, how do we define \"similar\"?\n",
    "\n",
    "Let's run a simulation to see this phenomenon in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3f1d9",
   "metadata": {},
   "source": [
    "## Seeing the Curse in Action (simulation)\n",
    "\n",
    "Let's generate random points in spaces of increasing dimensionality (from 1D to 1000D) and observe what happens.\n",
    "\n",
    "**Setup:** We create 100 random points in a $d$-dimensional unit hypercube $[0,1]^d$ and measure their distances from a fixed query point at the center (0.5, 0.5, ..., 0.5).\n",
    "\n",
    "**Key questions:**\n",
    "1. As $d$ increases, what happens to the *minimum* distance (closest point)?\n",
    "2. What happens to the *maximum* distance (farthest point)?\n",
    "3. What does this tell us about the ratio between nearest and farthest distances?\n",
    "\n",
    "**What to look for:** If the curse of dimensionality is real, we should see the minimum distance increasing (nearest neighbors getting farther away), while the ratio between min and max distances shrinks (distances becoming more uniform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_dataset(num_points, num_dimensions):\n",
    "    # Generate random points uniformly distributed in [0, 1]^n\n",
    "    return np.random.rand(num_points, num_dimensions)\n",
    "\n",
    "def calculate_distances(dataset, query_point):\n",
    "    # calculate Euclidean distances in an efficient way\n",
    "    return np.linalg.norm(dataset - query_point, axis=1)\n",
    "\n",
    "def run_experiment(dimensions, num_points=100):\n",
    "    min_distances = []\n",
    "    max_distances = []\n",
    "    \n",
    "    for dim in dimensions:\n",
    "        # Generate a random dataset and a query point\n",
    "        dataset = generate_random_dataset(num_points, dim)\n",
    "        \n",
    "        # define a query point\n",
    "        query_point = np.repeat(0.5, dim)\n",
    "        \n",
    "        # Calculate distances from the query point\n",
    "        distances = calculate_distances(dataset, query_point)\n",
    "        \n",
    "        # Record the min and max distances\n",
    "        min_distances.append(np.min(distances))\n",
    "        max_distances.append(np.max(distances))\n",
    "\n",
    "    return {'min_distances': min_distances, 'max_distances': max_distances}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "dimensions = [1, 2, 3, 10, 100, 1000]\n",
    "results = run_experiment(dimensions=dimensions)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dimensions, results['min_distances'], label='Minimum Distance', marker='o')\n",
    "plt.plot(dimensions, results['max_distances'], label='Maximum Distance', marker='o')\n",
    "plt.xlabel(\"Number of Dimensions (log scale)\")\n",
    "plt.ylabel(\"Distance (log scale)\")\n",
    "plt.title(\"Effect of Dimensionality on Distances\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xscale('log')  # Use log scale for better visualization\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69630a00",
   "metadata": {},
   "source": [
    "**Key observations from the simulation:**\n",
    "- As $d$ increases, the *minimum* distance grows substantially (your closest neighbor gets farther away)\n",
    "- The *maximum* distance also grows, but more slowly\n",
    "- The ratio between min and max distances approaches 1 (everything becomes roughly equidistant)\n",
    "- In 1000D, there's almost no meaningful difference between \"near\" and \"far\"\n",
    "\n",
    "**The bottom line:** Adding more features doesn't always help! Beyond some point, it actually makes our notion of \"similarity\" less useful.\n",
    "\n",
    "**The solution:** **Dimensionality reduction** -> intelligently compress high-dimensional data into fewer dimensions while preserving the important structure. This is where PCA comes in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
