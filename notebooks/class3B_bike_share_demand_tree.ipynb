{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b5892e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_log_error, make_scorer\n",
    "\n",
    "prng = np.random.RandomState(20250317)\n",
    "\n",
    "%precision 3\n",
    "pd.set_option('display.precision', 3)\n",
    "np.set_printoptions(suppress=True, precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eafdab",
   "metadata": {},
   "source": [
    "# Predict the demand for bike share using tree-based methods\n",
    "\n",
    "## Recreate the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "875dd548",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sample data\n",
    "bike_data = pd.read_csv(\"https://raw.githubusercontent.com/divenyijanos/ceu-ml/2025/data/bike_sharing_demand/bike_sample.csv\")\n",
    "features = bike_data.drop(columns=[\"count\", \"registered\", \"casual\"]).select_dtypes(include=np.number)\n",
    "label = bike_data[\"count\"]\n",
    "\n",
    "train_indices = pd.to_datetime(bike_data['datetime']).dt.day <= 15\n",
    "X_train = features[train_indices]\n",
    "X_test = features[~train_indices]\n",
    "y_train = label[train_indices]\n",
    "y_test = label[~train_indices]\n",
    "\n",
    "# feature engineered data\n",
    "def extractDtFeatures(df_with_datetime):\n",
    "    df_with_datetime['datetime'] = pd.to_datetime(df_with_datetime['datetime'], utc=True)\n",
    "    df_with_datetime['year'] = df_with_datetime['datetime'].dt.year\n",
    "    df_with_datetime['month'] = df_with_datetime['datetime'].dt.month\n",
    "    df_with_datetime['hour'] = df_with_datetime['datetime'].dt.hour\n",
    "    df_with_datetime['dayofweek'] = df_with_datetime['datetime'].dt.dayofweek\n",
    "\n",
    "extractDtFeatures(bike_data)\n",
    "\n",
    "feature_matrix = bike_data.drop(columns=[\"count\", \"registered\", \"casual\"]).select_dtypes(include=np.number)\n",
    "X_train_fe = feature_matrix[train_indices]\n",
    "X_test_fe = feature_matrix[~train_indices]\n",
    "\n",
    "\n",
    "# full data\n",
    "bike_full = pd.read_csv(\"https://raw.githubusercontent.com/divenyijanos/ceu-ml/2023/data/bike_sharing_demand/train.csv\")\n",
    "extractDtFeatures(bike_full)\n",
    "\n",
    "full_data_without_original_test = bike_full.loc[~bike_full.datetime.isin(bike_data.filter(X_test.index, axis=0)['datetime'])]\n",
    "full_data_without_original_test.shape\n",
    "\n",
    "X_full = full_data_without_original_test.drop(columns=[\"count\", \"registered\", \"casual\", \"datetime\"])\n",
    "y_full = full_data_without_original_test['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a66990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table for the various data sets\n",
    "data_objects = ['X_train', 'X_train_fe', 'y_train', 'X_full', 'y_full', 'X_test', 'X_test_fe', 'y_test']\n",
    "data_summary = {\n",
    "    name: {\n",
    "        'number_of_rows': locals()[name].shape[0], \n",
    "        'number_of_columns': locals()[name].shape[1] if len(locals()[name].shape) > 1 else 1\n",
    "    } \n",
    "    for name in data_objects\n",
    "}\n",
    "\n",
    "pd.DataFrame(data_summary).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4573c260",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8938d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateRMSLE(prediction, y_obs):\n",
    "    return np.sqrt(\n",
    "        np.mean(\n",
    "            (\n",
    "                np.log(np.where(prediction < 0, 0, prediction) + 1) -\n",
    "                np.log(y_obs + 1)\n",
    "            )**2\n",
    "        )\n",
    "    )\n",
    "\n",
    "class ResultCollector:\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        \n",
    "    def add_model(self, name, train_error, test_error):\n",
    "        \"\"\"Add or update a model's results.\"\"\"\n",
    "        self.results[name] = {\n",
    "            'Train RMSLE': train_error,\n",
    "            'Test RMSLE': test_error\n",
    "        }\n",
    "        return self.get_table()\n",
    "    \n",
    "    def get_table(self, style=True):\n",
    "        \"\"\"Get the results table with optional styling.\"\"\"\n",
    "        df = pd.DataFrame(self.results).T\n",
    "        if style:\n",
    "            return df.style.format(\"{:.3f}\").background_gradient(cmap='RdYlGn_r', axis=None)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b9c44a",
   "metadata": {},
   "source": [
    "## Benchmark: Linear model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d56fa99",
   "metadata": {},
   "source": [
    "We aim to build a pipeline that can process all datasets within a single loop. Since the exact dummy features depend on whether we are working with the raw or feature-engineered data, we need to define the encoding step carefully.\n",
    "\n",
    "To achieve this, we introduce a new method, `make_column_selector`, which selects columns based on a regex pattern or data types. We will apply one-hot encoding to all columns except those listed in `numeric_features`. The complex regex below simply ensures that columns named `temp`, `atemp`, `humidity`, or `windspeed` are excluded from encoding.\n",
    "\n",
    "If you’re not familiar with regular expressions, don’t worry—they won’t be required for any assignments. However, I highly recommend learning the basics, as they can be incredibly useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a85e454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['temp', 'atemp', 'humidity', 'windspeed']\n",
    "\n",
    "steps = [\n",
    "    (\"create_features\", ColumnTransformer([\n",
    "        (\"keep_numeric_features\", \"passthrough\", numeric_features),\n",
    "        (\"create_dummies\", OneHotEncoder(sparse_output=False, drop='first'), make_column_selector(pattern='^(?!(' + '|'.join(numeric_features) + ')$).*'))\n",
    "    ])),\n",
    "    (\"ols\", LinearRegression())\n",
    "]\n",
    "\n",
    "pipe_linear = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589be1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = [\"Linear\", \"Feature engineered linear\", \"Feature engineered linear large n\"]\n",
    "datasets = [\n",
    "    (X_train, y_train, X_test),\n",
    "    (X_train_fe, y_train, X_test_fe),\n",
    "    (X_full, y_full, X_test_fe)\n",
    "]\n",
    "\n",
    "results = ResultCollector()\n",
    "\n",
    "for model, (train_features, train_labels, test_features) in zip(models, datasets):\n",
    "\n",
    "    pipe_linear.fit(train_features, train_labels)\n",
    "\n",
    "    train_error = calculateRMSLE(pipe_linear.predict(train_features), train_labels)\n",
    "    test_error = calculateRMSLE(pipe_linear.predict(test_features), y_test)\n",
    "\n",
    "    results.add_model(model, train_error, test_error)\n",
    "\n",
    "results.get_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b39c78a",
   "metadata": {},
   "source": [
    "## Model #5: Tree\n",
    "\n",
    "**TODO**: Estimate a tree model on all the datasets and evaluate their perfomance. Add the results to the `results` object by using its `add_model()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facc58b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBA\n",
    "\n",
    "results.get_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33c2932",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(pipe_tree['tree'], feature_names = X_full.columns.to_list(), max_depth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9466367",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_deep_tree = Pipeline([('tree', DecisionTreeRegressor(max_depth=31, random_state=prng))])\n",
    "\n",
    "models = [\"Deep tree\", \"Feature engineered deep tree\", \"Feature engineered deep tree large n\"]\n",
    "\n",
    "for model, (train_features, train_labels, test_features) in zip(models, datasets):\n",
    "\n",
    "    pipe_deep_tree.fit(train_features, train_labels)\n",
    "\n",
    "    train_error = calculateRMSLE(pipe_deep_tree.predict(train_features), train_labels)\n",
    "    test_error = calculateRMSLE(pipe_deep_tree.predict(test_features), y_test)\n",
    "\n",
    "    results.add_model(model, train_error, test_error)\n",
    "\n",
    "results.get_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3024d684",
   "metadata": {},
   "source": [
    "**Lessons:**\n",
    "\n",
    "- A simple tree can capture complex (non-linear) relationships (we achieved much better performance than with the cumbersome LASSO without any tweaks). This does not mean that LASSO is useless, only that the tree seems to be a much better fit for this problem.\n",
    "- More data did not really help the shallow tree (`max_depth=5`) because it was not complex enough -- it did help the deep tree but also led to massive overfitting on the sample dataset (compare the test set performance of the two tree models on the sample dataset).\n",
    "- Actually, the deep tree overfits on all datasets as test error is always larger than the train error; however, it is less of a problem until the test error is still better than with the less complex model. It would be a good exercise to choose the `max_depth` parameter by cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c656f17",
   "metadata": {},
   "source": [
    "## Model #6: Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd43e4e8",
   "metadata": {},
   "source": [
    "### Default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1e3e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "pipe_rf = Pipeline([('random_forest', RandomForestRegressor(max_features=3))])\n",
    "\n",
    "models = [\"RF\", \"Feature engineered RF\", \"Feature engineered RF large n\"]\n",
    "\n",
    "for model, (train_features, train_labels, test_features) in zip(models, datasets):\n",
    "\n",
    "    pipe_rf.fit(train_features, train_labels)\n",
    "\n",
    "    train_error = calculateRMSLE(pipe_rf.predict(train_features), train_labels)\n",
    "    test_error = calculateRMSLE(pipe_rf.predict(test_features), y_test)\n",
    "\n",
    "    results.add_model(model, train_error, test_error)\n",
    "\n",
    "results.get_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b0423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf['random_forest'].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at single trees\n",
    "chosen_tree = pipe_rf['random_forest'].estimators_[0]\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(chosen_tree, feature_names = X_full.columns.to_list(), max_depth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc24f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_tree = pipe_rf['random_forest'].estimators_[1]\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(chosen_tree, feature_names = X_full.columns.to_list(), max_depth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd620dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "(chosen_tree.tree_.max_depth, chosen_tree.tree_.node_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b058e",
   "metadata": {},
   "source": [
    "**Lessons:**\n",
    "\n",
    "- Interestingly, the default `RandomForestRegressor` does not randomly choose from the features at each split (as `max_features = n_features`), it is equivalent to bagged trees. Also, there is no restricition on the size of the tree (`max_depth = None, max_leaf_nodes: None, min_samples_leaf = 1, min_samples_split: 2`).\n",
    "- Each tree in a random forest tends to memorize the training data. However, since each tree is trained on a bootstrapped sample—a randomly drawn subset with replacement—the aggregated prediction is less prone to overfitting. This demonstrates how bootstrapping inherently contributes to generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8869d5",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37df64a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 1000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [0.05, 0.5, 0.7, 1.0]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(1, 100, num = 5)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'bootstrap': bootstrap\n",
    "}\n",
    "random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b0b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE! This cell requires sklearn 1.4+\n",
    "# WARNING! This cell might take several minutes to run. Decrease `n_iter` to make it quicker (and less thorough)\n",
    "\n",
    "# hyper-parameter tuning - fit the models on the full sample\n",
    "rf = RandomForestRegressor(random_state=prng)\n",
    "\n",
    "# Random search of parameters, using 5 fold cross validation,\n",
    "# search across n_iter different combinations, and use all available cores, evaluate by RMSLE\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=10, cv=5, scoring='neg_root_mean_squared_log_error', verbose=2, random_state=prng, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_full, y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cab9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1366773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rf_random.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97694e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = calculateRMSLE(rf_random.best_estimator_.predict(X_full), y_full)\n",
    "test_error = calculateRMSLE(rf_random.best_estimator_.predict(X_test_fe), y_test)\n",
    "results.add_model(\"CV RF large n\", train_error, test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aac18f",
   "metadata": {},
   "source": [
    "## Model #7: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035e1fa",
   "metadata": {},
   "source": [
    "### Technical detour: category type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb2f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_features = ['season', 'holiday', 'workingday', 'weather', 'year', 'month', 'hour', 'dayofweek']\n",
    "X_full[dummy_features] = X_full[dummy_features].astype('category')\n",
    "X_full.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66305bd9",
   "metadata": {},
   "source": [
    "Why we did not deal with this so far? Because sklearn's implementation of trees [does not handle them differently](https://scikit-learn.org/stable/modules/tree.html#:~:text=and%20categorical%20data.-,However%2C%20the%20scikit%2Dlearn%20implementation%20does%20not%20support%20categorical%20variables%20for%20now.,-Other%20techniques%20are)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration\n",
    "pipe_tree.fit(X_full, y_full)\n",
    "\n",
    "train_error = calculateRMSLE(pipe_tree.predict(X_full), y_full)\n",
    "test_error = calculateRMSLE(pipe_tree.predict(X_test_fe), y_test)\n",
    "\n",
    "results.add_model(\"Tree large n categories\", train_error, test_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851f5af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(pipe_tree[\"tree\"], feature_names = X_full.columns.to_list(), max_depth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8203322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted tree: xgboost\n",
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBRegressor(enable_categorical=True).fit(X_full, y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce31e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = calculateRMSLE(xgb_model.predict(X_full), y_full)\n",
    "test_error = calculateRMSLE(xgb_model.predict(X_test_fe), y_test)\n",
    "\n",
    "[\"XGB\", train_error, test_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4f89c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.get_booster().trees_to_dataframe().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for xgboost model\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(enable_categorical=True)\n",
    "\n",
    "def safe_rmsle(y_true, y_pred):\n",
    "    y_pred = np.maximum(y_pred, 0)  # clip predictions at 0\n",
    "    return -np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    estimator=xgb_model, param_distributions=param_grid, \n",
    "    n_iter=100, cv=5, scoring=make_scorer(safe_rmsle), verbose=2, random_state=prng, n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_random.fit(X_full, y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe763a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_xgb_model = xgb_random.best_estimator_\n",
    "\n",
    "train_error = calculateRMSLE(best_xgb_model.predict(X_full), y_full)\n",
    "test_error = calculateRMSLE(best_xgb_model.predict(X_test_fe), y_test)\n",
    "\n",
    "[\"XGB Tuned\", train_error, test_error]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4f0a9",
   "metadata": {},
   "source": [
    "**Lessons:**\n",
    "\n",
    "- \"Both xgboost and gbm follows the principle of gradient boosting. There are however, the difference in modeling details. Specifically, xgboost used a more regularized model formalization to control over-fitting, which gives it better performance.\" [Documentation](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)\n",
    "- xgboost can handle categorical values, resulting in rules defined by sets instead of simple numerical cuts\n",
    "- for this particular problem, Random Forest performed better than xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b16601",
   "metadata": {},
   "source": [
    "## Submit to Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf36ba",
   "metadata": {},
   "source": [
    "Kaggle provides a test set without any labels. Our task is to predict these labels and submit to Kaggle which will evaluate our model's performance based on its evaluation metric (RMSLE in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "99d9ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_test = pd.read_csv(\"https://raw.githubusercontent.com/divenyijanos/ceu-ml/2025/data/bike_sharing_demand/test.csv\")\n",
    "extractDtFeatures(bike_test)\n",
    "\n",
    "# ensure we comply with the formatting requirement by Kaggle\n",
    "to_submit = pd.DataFrame({\n",
    "    'datetime': bike_test.datetime.dt.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'count': pipe_rf.predict(bike_test.drop(columns=[\"datetime\"]))\n",
    "})\n",
    "\n",
    "to_submit.to_csv(\"../data/bike_sharing_demand/submission1.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CEU-ML)",
   "language": "python",
   "name": "ceu-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
