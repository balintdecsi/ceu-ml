{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b5892e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LassoCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "prng = np.random.RandomState(20250317)\n",
    "\n",
    "%precision 3\n",
    "pd.set_option('display.precision', 3)\n",
    "np.set_printoptions(suppress=True, precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49917a3",
   "metadata": {},
   "source": [
    "# Predict the demand for bike share using linear models\n",
    "\n",
    "Our goal is to predict demand for bike share based on [this](https://www.kaggle.com/c/bike-sharing-demand) Kaggle task.\n",
    "Kaggle provides two data sets: a labelled train data and an unlabelled test data.\n",
    "We have to use the train data to predict labels for the test data. The data consists of hourly rental data spanning two years. The training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month.\n",
    "Kaggle won't give us the labels just a score we achieved on the test set.\n",
    "\n",
    "\n",
    "### Know your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be303e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data = pd.read_csv(\"https://raw.githubusercontent.com/divenyijanos/ceu-ml/2025/data/bike_sharing_demand/bike_sample.csv\")\n",
    "bike_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18f09b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bike_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4733741",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc153612",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data['datetime'] = pd.to_datetime(bike_data['datetime'])\n",
    "bike_2011 = bike_data[bike_data['datetime'].dt.year == 2011]\n",
    "daily_counts = bike_2011.groupby(bike_2011['datetime'].dt.date)['count'].sum()\n",
    "dates = daily_counts.index\n",
    "counts = daily_counts.values\n",
    "\n",
    "plt.bar(dates, counts, color='darkblue')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Rental Count\")\n",
    "plt.title(\"Daily Rentals for 2011\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b02fd8-2493-4b26-bffe-0f834502868f",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66a3649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split keeping numeric features\n",
    "\n",
    "features = bike_data.drop(columns=['count']).select_dtypes(include=np.number)\n",
    "label = bike_data['count']\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2, random_state=prng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e820911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a better train-test split\n",
    "\n",
    "train_indices = pd.to_datetime(bike_data['datetime']).dt.day <= 15\n",
    "X_train = features[train_indices]\n",
    "X_test = features[~train_indices]\n",
    "y_train = label[train_indices]\n",
    "y_test = label[~train_indices]\n",
    "\n",
    "print(f\"Resulting size of the test is: {y_test.shape[0] / bike_data.shape[0]:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbbe142-4fba-4c95-a278-38b10b3dcfa6",
   "metadata": {},
   "source": [
    "### Evaluation function\n",
    "\n",
    "**TODO**: Write a loss function that calculates the Root Mean Squared Log Error (RMSLE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d02a113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a loss function \n",
    "def calculateRMSLE(prediction, y_obs):\n",
    "    # TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c6f0d-ffdc-4549-af40-136f3e5a2c76",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "**TODO**: Estimate a _very_ simple benchmark model (average), and evaluate its performance on both the train and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1cd3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate benchmark model\n",
    "benchmark = # TBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eae4bb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to keep track of the results\n",
    "class ResultCollector:\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        \n",
    "    def add_model(self, name, train_error, test_error):\n",
    "        \"\"\"Add or update a model's results.\"\"\"\n",
    "        self.results[name] = {\n",
    "            'Train RMSLE': train_error,\n",
    "            'Test RMSLE': test_error\n",
    "        }\n",
    "        return self.get_table()\n",
    "    \n",
    "    def get_table(self, style=True):\n",
    "        \"\"\"Get the results table with optional styling.\"\"\"\n",
    "        df = pd.DataFrame(self.results).T\n",
    "        if style:\n",
    "            return df.style.format(\"{:.3f}\").background_gradient(cmap='RdYlGn_r', axis=None)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84cb8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ResultCollector()\n",
    "results.add_model(\"Benchmark\", calculateRMSLE(benchmark, y_train), calculateRMSLE(benchmark, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806c2afc-4cf8-461d-8c6c-00acce0c3256",
   "metadata": {},
   "source": [
    "### Model #1: Group averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a61f91-135b-4cb3-b18c-e4e45f46588e",
   "metadata": {},
   "source": [
    "#### Statistics recap: linear regression and averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f23030-6b61-4c4d-8c93-10cc74b829ba",
   "metadata": {},
   "source": [
    "If you fit a linear regression model using a **dummy variable** without any other features, the model will predict the average outcome for the groups represented by the dummy variables. A binary variable with numeric values 0 and 1 behaves as a dummy variable without any further transformation. The estimated intercept of the linear regression captures the average of the reference category (for which the binary variable takes the value of 0) while the coefficient expressed the difference between the averages of the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c0f3c-7b95-40d9-8f06-90ce3b4f9d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustration on a single binary variable\n",
    "lin_reg_simple_dummy = LinearRegression().fit(X_train[['holiday']], y_train)\n",
    "prediction = lin_reg_simple_dummy.predict(X_train[['holiday']])\n",
    "\n",
    "joint_data = pd.DataFrame({\n",
    "    'dummy': X_train['holiday'].values,\n",
    "    'y': y_train,\n",
    "    'prediction': prediction\n",
    "})\n",
    "joint_data.groupby('dummy').agg({'y': 'mean', 'prediction': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15a6c2c-767e-4168-b1cf-f9c328c060ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare it to estimated coefficients\n",
    "[lin_reg_simple_dummy.intercept_, lin_reg_simple_dummy.intercept_ + lin_reg_simple_dummy.coef_[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b775a4e-429c-471b-802b-24617c6b3ba7",
   "metadata": {},
   "source": [
    "For a categorical variable with **multiple categories**, we have to ensure to represent each category level by a dummy variable (called one-hot encoding achieved by `pd.get_dummies()` or `OneHotEncoder`).\n",
    "To avoid multicollinearity issues, you need to omit one of the dummy variables. Including all of them in the model would lead to redundant information because the value of one dummy variable can be predicted from the values of the other dummy variables. By omitting one dummy variable, you set it as the reference category, and the coefficients of the remaining dummy variables represent the difference in the outcome variable between each category and the reference category. With the constant term, you estimate as many coefficients as there are category levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedfb092-0599-4186-b3e7-797a1b492c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustration on a multi-level categorical variable\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "\n",
    "lin_reg_multicategory_dummy = Pipeline([\n",
    "    ('dummify', one_hot_encoder),\n",
    "    ('ols', LinearRegression())\n",
    "])\n",
    "\n",
    "lin_reg_multicategory_dummy.fit(X_train[['season']], y_train)\n",
    "prediction = lin_reg_multicategory_dummy.predict(X_train[['season']])\n",
    "\n",
    "joint_data = pd.DataFrame({\n",
    "    'season': X_train['season'].values,\n",
    "    'y': y_train,\n",
    "    'prediction': prediction\n",
    "})\n",
    "\n",
    "joint_data.groupby('season').agg({'y': 'mean', 'prediction': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a238a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data which is used for fitting\n",
    "lin_reg_multicategory_dummy['dummify'].fit_transform(X_train[['season']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3509872-6278-40d5-9401-0426b0c27c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare it to estimated coefficients\n",
    "lm_model = lin_reg_multicategory_dummy['ols']\n",
    "[lm_model.intercept_] + [lm_model.intercept_ + coef for coef in lm_model.coef_]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d83984c-553b-4df5-bc54-1a152f44c326",
   "metadata": {},
   "source": [
    "If you have **multiple categorical variables**, life gets complicated. To ensure that you estimate as many coefficients as there are combinations of categories to capture the average of each group, we also need to include the _interaction_ of the dummies. You can achieve this with `PolynomialFeatures`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f2d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration on multiple categorical variables\n",
    "\n",
    "create_interactions = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "\n",
    "lin_reg_complex_dummies =Pipeline([\n",
    "    ('dummify', one_hot_encoder),\n",
    "    ('create_interactions', create_interactions),\n",
    "    ('ols', LinearRegression())\n",
    "])\n",
    "\n",
    "lin_reg_complex_dummies.fit(X_train[['season', 'workingday']], y_train)\n",
    "prediction = lin_reg_complex_dummies.predict(X_train[['season', 'workingday']])\n",
    "\n",
    "joint_data = pd.concat([\n",
    "    X_train[['season', 'workingday']],\n",
    "    pd.DataFrame({\n",
    "        'y': y_train,\n",
    "        'prediction': prediction\n",
    "    })\n",
    "], axis=1)\n",
    "joint_data.groupby(['workingday', 'season']).agg({'y': 'mean', 'prediction': 'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987b0338-ea01-422d-919a-22806d35b83e",
   "metadata": {},
   "source": [
    "It indeed predicts group averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b550486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have more coefficients than the number of combinations of categories -- some of the coffients are zero (the overall effect is the same as the intercept)\n",
    "[lin_reg_complex_dummies['ols'].intercept_] + [lin_reg_complex_dummies['ols'].intercept_ + coef for coef in lin_reg_complex_dummies['ols'].coef_]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ca779",
   "metadata": {},
   "source": [
    "\n",
    "However, if some of your categorical variables are multilevel, the interaction of these dummies within the same category (e.g. `season_2 * season_3`) will be constant zero, and you will get a linearly-dependent (rank-deficient) feature matrix. The optimization algorithm of the `LinearRegression` will give you a solution anyway estimating 0-s for the corresponding coefficients. It might be better to exclude variables with zero variance right away using the `VarianceThreshold` method (which defaults to the threshold of zero variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a7e6bb-e4b5-4dd5-9642-0addc4e03ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_complex_dummies = Pipeline([\n",
    "    ('dummify', one_hot_encoder),\n",
    "    ('create_interactions', create_interactions),\n",
    "    ('drop_zero_variance', VarianceThreshold()),\n",
    "    ('ols', LinearRegression())\n",
    "])\n",
    "\n",
    "lin_reg_complex_dummies.fit(X_train[['season', 'workingday']], y_train)\n",
    "prediction = lin_reg_complex_dummies.predict(X_train[['season', 'workingday']])\n",
    "\n",
    "joint_data = pd.concat([\n",
    "    X_train[['season', 'workingday']],\n",
    "    pd.DataFrame({\n",
    "        'y': y_train,\n",
    "        'prediction': prediction\n",
    "    })\n",
    "], axis=1)\n",
    "joint_data.groupby(['workingday', 'season']).agg({'y': 'mean', 'prediction': 'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4bafc8-1579-42e2-a1d9-b548fd8591e2",
   "metadata": {},
   "source": [
    "#### Technical detour: feature transformation within Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a16ac48-da9c-4d77-8a35-d40f05eb74d1",
   "metadata": {},
   "source": [
    "If you want to apply specific transformations on some columns, you can use `ColumnTransformer`. A `ColumnTransformer` takes a list of transformation and some optional parameters (like what to do with the `remainder` columns that were not specified in the transformation steps; defaults to `\"drop\"`, change to `\"passthrough\"` if you want to keep them). Each transformation consists of a three-element tuple: a name (you name it), the operation, and the columns the operation needs to be applied (given by the list of name, indices, etc.).\n",
    "\n",
    "While `Pipeline` executes the steps sequentially, `ColumnTransformer` applies the listed operations at once on different sets of columns.\n",
    "\n",
    "See [this Medium post](https://towardsdatascience.com/simplifying-machine-learning-model-development-with-columntransformer-pipeline-f09ffb04ca6b) for a more detailed explanation.\n",
    "\n",
    "Here we would like to apply multiple steps sequentially on a specific set of the original columns in our training data, so we will pass a `Pipeline` to the `ColumnTransformer`. Then, we we would like to estimate a model, so we build another `Pipeline` for the whole process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a14c2bd-31ac-4843-a77f-58c4581faab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_features = [\"season\", \"workingday\"]\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False, drop=\"first\")\n",
    "\n",
    "create_categorical_features = Pipeline([\n",
    "    (\"dummify\", one_hot_encoder),\n",
    "    (\"create_interactions\", create_interactions),\n",
    "    (\"drop_zero_variance\", VarianceThreshold())\n",
    "])\n",
    "\n",
    "pipe_whole_process = Pipeline([\n",
    "    (\"create_features\", ColumnTransformer([(\"choose_and_transform_features\", create_categorical_features, dummy_features)])),\n",
    "    (\"ols\", LinearRegression())\n",
    "])\n",
    "pipe_whole_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421dff82-2215-4501-b93b-a2e7d7604a2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe_whole_process.fit(X_train, y_train)  # note that we fit the whole X_train, not just the selected columns, as the ColumnTransformer will do the selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ab2d2-ee35-4a20-abde-45520dd6c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double-check we got the same result\n",
    "(\n",
    "    calculateRMSLE(lin_reg_complex_dummies.predict(X_train[['season', 'workingday']]), y_train),\n",
    "    calculateRMSLE(pipe_whole_process.predict(X_train), y_train),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a4f0cc-4644-426e-aaba-eb932bf8445a",
   "metadata": {},
   "source": [
    "#### Estimate model #1: group averages by weather, workingday and holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd3a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate & evaluate model #1\n",
    "dummy_features = [\"season\", \"workingday\", \"holiday\"]\n",
    "\n",
    "steps = [\n",
    "    (\"create_features\", ColumnTransformer([(\"choose_and_transform_features\", create_categorical_features, dummy_features)])),\n",
    "    (\"ols\", LinearRegression())\n",
    "]\n",
    "\n",
    "pipe_group_avg = Pipeline(steps)\n",
    "pipe_group_avg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "train_error = calculateRMSLE(pipe_group_avg.predict(X_train), y_train)\n",
    "test_error = calculateRMSLE(pipe_group_avg.predict(X_test), y_test)\n",
    "\n",
    "results.add_model(\"Group averages\", train_error, test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb673e-cf28-46b6-9570-c531a9f5e1ee",
   "metadata": {},
   "source": [
    "### Model #2: Group averages with weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6928067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model #2: Group averages with weather\n",
    "dummy_features = ['season', 'holiday', 'workingday', 'weather']\n",
    "numeric_features = ['temp', 'atemp', 'humidity', 'windspeed']\n",
    "\n",
    "steps = [\n",
    "    (\"create_features\", ColumnTransformer([\n",
    "        (\"choose_and_transform_dummy_features\", create_categorical_features, dummy_features),\n",
    "        (\"keep_numeric_features\", \"passthrough\", numeric_features)\n",
    "    ])),\n",
    "    (\"ols\", LinearRegression())\n",
    "]\n",
    "\n",
    "pipe_group_avg_with_weather = Pipeline(steps)\n",
    "pipe_group_avg_with_weather.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a440fb-dd26-4a2b-a507-d12a29e1a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.add_model(\n",
    "    'Group avgs with weather',\n",
    "    calculateRMSLE(pipe_group_avg_with_weather.predict(X_train), y_train),\n",
    "    calculateRMSLE(pipe_group_avg_with_weather.predict(X_test), y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0491c4-3dc6-49cd-a445-d6012dcf865e",
   "metadata": {},
   "source": [
    "### Model #3: Very flexible linear with polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    (\"create_features\", ColumnTransformer([\n",
    "        (\"create_dummy_features\", one_hot_encoder, dummy_features), # we will create interactions later across all features\n",
    "        (\"keep_numeric_features\", \"passthrough\", numeric_features)\n",
    "    ])),\n",
    "    (\"4_degree_poly\", PolynomialFeatures(degree=4, include_bias=False)),\n",
    "    (\"drop_zero_variance\", VarianceThreshold()),\n",
    "    (\"ols\", LinearRegression())\n",
    "]\n",
    "\n",
    "pipe_flexible_linear = Pipeline(steps)\n",
    "pipe_flexible_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0dcbab-88ee-4e8f-81af-6decf4295bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_flexible_linear.fit(X_train, y_train)\n",
    "train_error = calculateRMSLE(pipe_flexible_linear.predict(X_train), y_train)\n",
    "test_error = calculateRMSLE(pipe_flexible_linear.predict(X_test), y_test)\n",
    "\n",
    "results.add_model(\"Flexible linear\", train_error, test_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194cdb5c-660b-46b6-9278-e06a4ad5c293",
   "metadata": {},
   "source": [
    "### Model #4: Improve with Lasso\n",
    "\n",
    "**TODO**: Improve Model#3 by estimating a cross-validated Lasso on the expanded (flexible) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b6394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model #4: improve with Lasso\n",
    "steps = [\n",
    "    # TBA\n",
    "]\n",
    "pipe_lasso = Pipeline(steps)\n",
    "\n",
    "pipe_lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8359c9-2bc1-4afa-8235-952ed60aa2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = calculateRMSLE(pipe_lasso.predict(X_train), y_train)\n",
    "test_error = calculateRMSLE(pipe_lasso.predict(X_test), y_test)\n",
    "\n",
    "results.add_model(\"Flexible LASSO\", train_error, test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1ce63-4325-435b-b5a0-c6bfb16c1402",
   "metadata": {},
   "source": [
    "**Lessons:**\n",
    "\n",
    "- Gradually adding more information present in the training improves our models' performance.\n",
    "- Being very flexible without any penalty for the complexity leads to overfitting (test error >> train error).\n",
    "- Choosing a method with regularization (and tune the hyperparameter automatically by `LassoCV`) is able to exploit flexibility without overfitting - however, the performance gain of flexibility is small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb080379",
   "metadata": {},
   "source": [
    "## Improve the models\n",
    "\n",
    "### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671ee3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_predictions = pipe_group_avg_with_weather.predict(X_test)\n",
    "lasso_predictions = pipe_lasso.predict(X_test)\n",
    "\n",
    "plt.scatter(y_test, linear_predictions, label='Linear', alpha=0.5)\n",
    "plt.scatter(y_test, lasso_predictions, label='Lasso', alpha=0.5)\n",
    "plt.axline((1, 1), slope=1, linestyle='dashed', color='red')\n",
    "plt.xlabel('Observed')\n",
    "plt.ylabel('Predicted')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data[bike_data['count'] < 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3423e8",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51559758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDtFeatures(df_with_datetime):\n",
    "    df_with_datetime['datetime'] = pd.to_datetime(df_with_datetime['datetime'], utc=True)\n",
    "    df_with_datetime['year'] = df_with_datetime['datetime'].dt.year\n",
    "    df_with_datetime['month'] = df_with_datetime['datetime'].dt.month\n",
    "    df_with_datetime['hour'] = df_with_datetime['datetime'].dt.hour\n",
    "    df_with_datetime['dayofweek'] = df_with_datetime['datetime'].dt.dayofweek\n",
    "\n",
    "\n",
    "extractDtFeatures(bike_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0729e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = bike_data.drop(columns=[\"count\", \"registered\", \"casual\"]).select_dtypes(include=np.number)\n",
    "\n",
    "X_train_fe = feature_matrix[train_indices]\n",
    "X_test_fe = feature_matrix[~train_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26acd7-0496-4ff4-afe7-2845ecc01cbd",
   "metadata": {},
   "source": [
    "#### Linear (FE)\n",
    "\n",
    "We created many new categorical variables. Creating all the interactions would mean lots of parameters and we would be back in the \"very flexible\" scenario. Let's estimate a simpler linear model instead, where we only include the dummy variables but not their interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_features = ['season', 'holiday', 'workingday', 'weather', 'year', 'month', 'hour', 'dayofweek']\n",
    "\n",
    "steps = [\n",
    "    (\"create_features\", ColumnTransformer([\n",
    "        (\"create_dummies\", one_hot_encoder, dummy_features),\n",
    "        (\"keep_numeric_features\", \"passthrough\", numeric_features)\n",
    "    ])),\n",
    "    (\"ols\", LinearRegression())\n",
    "]\n",
    "\n",
    "pipe_linear = Pipeline(steps)\n",
    "pipe_linear.fit(X_train_fe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a38490-445c-4e71-a912-2d8fe48e16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = calculateRMSLE(pipe_linear.predict(X_train_fe), y_train)\n",
    "test_error = calculateRMSLE(pipe_linear.predict(X_test_fe), y_test)\n",
    "\n",
    "results.add_model(\"Feature engineered linear\", train_error, test_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d918083",
   "metadata": {},
   "source": [
    "#### Lasso (FE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e2884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "steps = [\n",
    "    (\"dummify_selected_columns\", ColumnTransformer([\n",
    "        (\"dummify\", one_hot_encoder, dummy_features),\n",
    "        (\"scale\", StandardScaler(), numeric_features)\n",
    "    ])),\n",
    "    (\"2_degree_poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"lasso\", LassoCV(random_state=prng))\n",
    "]\n",
    "pipe_lasso = Pipeline(steps)\n",
    "pipe_lasso.fit(X_train_fe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f1a29-a12e-4f81-a158-ed083c3ed57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = calculateRMSLE(pipe_lasso.predict(X_train_fe), y_train)\n",
    "test_error = calculateRMSLE(pipe_lasso.predict(X_test_fe), y_test)\n",
    "\n",
    "results.add_model(\"Feature engineered Lasso\", train_error, test_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2551cde5-5a12-4b7a-87fd-0e947a324ef6",
   "metadata": {},
   "source": [
    "**Lessons:**\n",
    "\n",
    "- You should always look for more information hidden in your data.\n",
    "- Extracting information hidden in the non-numeric `datetime` column resulted in a huge improvement for both OLS and Lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc1ebae",
   "metadata": {},
   "source": [
    "### Collect more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf7626",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_full = pd.read_csv(\"https://raw.githubusercontent.com/divenyijanos/ceu-ml/2023/data/bike_sharing_demand/train.csv\")\n",
    "bike_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075a6257",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9090a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "extractDtFeatures(bike_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5fc951-b41d-42cc-9fb5-d55003a263a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_full.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e5298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the test set remains intact -> all the new data goes into the training set\n",
    "full_data_without_original_test = bike_full.loc[~bike_full.datetime.isin(bike_data.filter(X_test.index, axis=0)['datetime'])]\n",
    "full_data_without_original_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be4fe178",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = full_data_without_original_test.drop(columns=[\"count\", \"registered\", \"casual\", \"datetime\"])\n",
    "y_full = full_data_without_original_test['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd456628",
   "metadata": {},
   "source": [
    "#### Group averages with weather (full data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe_group_avg_with_weather.fit(X_full, y_full)\n",
    "\n",
    "train_error = calculateRMSLE(pipe_group_avg_with_weather.predict(X_full), y_full)\n",
    "test_error = calculateRMSLE(pipe_group_avg_with_weather.predict(X_test), y_test)\n",
    "\n",
    "results.add_model(\"Group avgs with weather large n\", train_error, test_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058c7e91",
   "metadata": {},
   "source": [
    "#### Linear model (FE, full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe_linear.fit(X_full, y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88f4dce-f436-43d2-9bfe-5bb494e5c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "train_error = calculateRMSLE(pipe_linear.predict(X_full), y_full)\n",
    "test_error = calculateRMSLE(pipe_linear.predict(X_test_fe), y_test)\n",
    "\n",
    "results.add_model(\"Feature engineered linear large n\", train_error, test_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07490b85",
   "metadata": {},
   "source": [
    "#### Lasso (FE, Full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd82502",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe_lasso.fit(X_full, y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda4f57-103d-42b4-a143-e746c69df1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "train_error = calculateRMSLE(pipe_lasso.predict(X_full), y_full)\n",
    "test_error = calculateRMSLE(pipe_lasso.predict(X_test_fe), y_test)\n",
    "\n",
    "results.add_model(\"Feature engineered Lasso large n\", train_error, test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e6315b-66d5-4dd9-926d-2e3ddf0eecb2",
   "metadata": {},
   "source": [
    "**Lessons:**\n",
    "\n",
    "- Collecting more samples from the same domain could help\n",
    "- But only if the model is flexible enough to capture new (more subtle) patterns. A simple average is usually stable enough once you have 20 observations so collecting more won't have much impact. However, models that allow for complexity, such as the lasso on complex transformations, could benefit from the new set of training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CEU-ML)",
   "language": "python",
   "name": "ceu-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
