{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a255e19b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T09:13:19.214767Z",
     "iopub.status.busy": "2026-02-23T09:13:19.214682Z",
     "iopub.status.idle": "2026-02-23T09:13:20.726536Z",
     "shell.execute_reply": "2026-02-23T09:13:20.725961Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_openml, make_moons\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, confusion_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting settings\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbc8191",
   "metadata": {},
   "source": [
    "# Class 3: Clustering\n",
    "\n",
    "## Introduction: The Challenge of Finding Structure\n",
    "\n",
    "**Recall from Class 1**: We explored the **curse of dimensionality** - as dimensions increase:\n",
    "1. **Sparsity**: Data moves to the edges of the space\n",
    "2. **Distance concentration**: All distances become similar\n",
    "\n",
    "**Recall from Class 2**: We learned **PCA** as a tool for dimensionality reduction - preserving information while reducing the number of features.\n",
    "\n",
    "**Today's question**: Can we use these insights to **discover groups** in data?\n",
    "\n",
    "### What is Clustering?\n",
    "\n",
    "**Clustering** is the task of grouping similar observations together **without knowing the \"correct\" groupings beforehand**. This is **unsupervised learning** - we have no labels!\n",
    "\n",
    "**Key insight**: Clustering relies on **distance-based similarity** (from Class 1):\n",
    "- **Small distance** = similar observations (should be in same cluster)\n",
    "- **Large distance** = dissimilar observations (should be in different clusters)\n",
    "\n",
    "**The curse strikes again!** If distances become meaningless in high dimensions, clustering will fail!\n",
    "\n",
    "Let's start with a real high-dimensional dataset to see this problem in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa9eb87",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality in Clustering: MNIST\n",
    "\n",
    "### Real-World Example: Handwritten Digits\n",
    "\n",
    "**MNIST** is a classic image dataset containing 70,000 handwritten digit images (0-9), each 28Ã—28 pixels.\n",
    "\n",
    "**From images to numbers**: Each grayscale pixel becomes a feature (0 = white, 255 = black)\n",
    "- **28 Ã— 28 = 784 features** per image (now we'll handle them as independent features)\n",
    "- **High-dimensional**: Each image is a point in 784-dimensional space!\n",
    "\n",
    "**Our goal**: Can K-Means clustering discover the 10 digit classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134954c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T09:13:20.728465Z",
     "iopub.status.busy": "2026-02-23T09:13:20.728298Z",
     "iopub.status.idle": "2026-02-23T09:13:22.429358Z",
     "shell.execute_reply": "2026-02-23T09:13:22.428976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load MNIST - the classic handwritten digits dataset\n",
    "print(\"Loading MNIST (28x28 images)... this may take a moment on first run...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "X_mnist = mnist.data.astype(np.float32)\n",
    "y_mnist = mnist.target.astype(int)\n",
    "\n",
    "print(f\"\\nMNIST Dataset:\")\n",
    "print(f\"  Shape: {X_mnist.shape} (samples Ã— pixels)\")\n",
    "print(f\"  Classes: {np.unique(y_mnist)} (digits 0-9)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7a714",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T09:13:22.430602Z",
     "iopub.status.busy": "2026-02-23T09:13:22.430503Z",
     "iopub.status.idle": "2026-02-23T09:13:22.699338Z",
     "shell.execute_reply": "2026-02-23T09:13:22.698952Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize some digits\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 2))\n",
    "fig.suptitle('MNIST Digits: Sample images for each class', fontsize=14)\n",
    "\n",
    "for i in range(10):\n",
    "    # Show example digit\n",
    "    idx = np.where(y_mnist == i)[0][0]\n",
    "    axes[i].imshow(X_mnist[idx].reshape(28, 28), cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'Digit {i}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29301205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify uninformative pixels: show pixel variance across all images\n",
    "pixel_variance = X_mnist.var(axis=0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "fig.suptitle('Many MNIST pixels are uninformative (low variance)', fontsize=14)\n",
    "\n",
    "# Show variance heatmap\n",
    "axes[0].imshow(pixel_variance.reshape(28, 28), cmap='hot')\n",
    "axes[0].set_title('Pixel Variance\\n(dark = uninformative)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Count uninformative pixels\n",
    "low_var_threshold = 100  # pixels with variance < 100\n",
    "n_low_var = (pixel_variance < low_var_threshold).sum()\n",
    "axes[1].text(0.5, 0.5, f'{n_low_var}/784\\npixels (on the borders) have\\nlow variance\\n({n_low_var/784:.0%} noise!)', \n",
    "             ha='center', va='center', fontsize=14, transform=axes[1].transAxes)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dda756",
   "metadata": {},
   "source": [
    "Shall we standardize?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f509d18",
   "metadata": {},
   "source": [
    "### The Curse Manifests: Distance Concentration\n",
    "\n",
    "**Recall from Class 1**: In high dimensions, the ratio of maximum to minimum distances approaches 1.\n",
    "\n",
    "Let's verify this happens in MNIST! We'll keep 200 images and look at their pairwise distances with different number of features (pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95bf8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample images\n",
    "np.random.seed(20260225)\n",
    "sample_size = 200\n",
    "sample_idx = np.random.choice(len(X_mnist), sample_size, replace=False)\n",
    "X_sample = X_mnist[sample_idx]\n",
    "X_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b62f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise distances\n",
    "from scipy.spatial.distance import pdist\n",
    "import time\n",
    "\n",
    "def run_mnist_pairwise_experiment(X_sample, dimensions, n_bootstrap=200):\n",
    "    \"\"\"\n",
    "    For each dimension count, randomly select that many features from MNIST,\n",
    "    then compute ALL pairwise distances and find min/max.\n",
    "    Note: pdist computes n*(n-1)/2 distances, so we use fewer samples.\n",
    "    \"\"\"\n",
    "    np.random.seed(20260225) # reset\n",
    "    \n",
    "    results = {'dimensions': dimensions, \n",
    "               'min_mean': [], 'min_lower': [], 'min_upper': [],\n",
    "               'max_mean': [], 'max_lower': [], 'max_upper': []}\n",
    "    \n",
    "    for dim in dimensions:\n",
    "        min_dists = []\n",
    "        max_dists = []\n",
    "        \n",
    "        for boot in range(n_bootstrap):\n",
    "            # Randomly select 'dim' features (pixels)\n",
    "            selected_dims = np.random.choice(784, min(dim, 784), replace=False)\n",
    "            X_subset = X_sample[:, selected_dims]\n",
    "            \n",
    "            # Compute ALL pairwise distances\n",
    "            pairwise_distances = pdist(X_subset, metric='euclidean')\n",
    "\n",
    "            min_dists.append(np.min(pairwise_distances))\n",
    "            max_dists.append(np.max(pairwise_distances))\n",
    "        \n",
    "        # Calculate mean and 90% confidence intervals\n",
    "        results['min_mean'].append(np.mean(min_dists))\n",
    "        results['min_lower'].append(np.percentile(min_dists, 5))\n",
    "        results['min_upper'].append(np.percentile(min_dists, 95))\n",
    "        \n",
    "        results['max_mean'].append(np.mean(max_dists))\n",
    "        results['max_lower'].append(np.percentile(max_dists, 5))\n",
    "        results['max_upper'].append(np.percentile(max_dists, 95))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run experiment with pairwise distances\n",
    "dimensions = [5, 10, 25, 50, 100, 200, 400, 784]\n",
    "\n",
    "print(f\"Computing pairwise distances for {sample_size} samples...\")\n",
    "start_time = time.time()\n",
    "\n",
    "results_pairwise = run_mnist_pairwise_experiment(X_sample, dimensions, n_bootstrap=30)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"â±ï¸  Completed in {elapsed:.1f} seconds\")\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "dims = results_pairwise['dimensions']\n",
    "\n",
    "# Min and Max pairwise distances\n",
    "ax.plot(dims, results_pairwise['min_mean'], 'o-', linewidth=2.5, markersize=8, label='Min Pairwise Distance')\n",
    "ax.fill_between(dims, results_pairwise['min_lower'], results_pairwise['min_upper'], alpha=0.2)\n",
    "\n",
    "ax.plot(dims, results_pairwise['max_mean'], 'o-', linewidth=2.5, markersize=8, label='Max Pairwise Distance')\n",
    "ax.fill_between(dims, results_pairwise['max_lower'], results_pairwise['max_upper'], alpha=0.2)\n",
    "\n",
    "ax.set_xlabel('Number of Dimensions (log scale)', fontsize=12)\n",
    "ax.set_ylabel('Pairwise Distance (log scale)', fontsize=12)\n",
    "ax.set_title('MNIST: Min/Max Pairwise Distance\\n(between any two points, shaded = 90% confidence band)', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fa905",
   "metadata": {},
   "source": [
    "### Clustering with KMeans\n",
    "\n",
    "Now let's apply K-Means clustering to our MNIST data to see how well it can discover the natural groupings (digits 0-9).\n",
    "\n",
    "**What we're doing:**\n",
    "1. **Sample 200 images** randomly from the full MNIST dataset\n",
    "2. **Run K-Means with K=10** clusters (matching the 10 digit classes), using `n_init=10` (10 random initializations to find the best solution) -- with the same `sklearn` pattern we already saw in action: `define` â†’ `fit` â†’ `predict`\n",
    "3. **Evaluate the clustering** using two approaches:\n",
    "\n",
    "**Confusion Matrix**: Shows how K-Means clusters align with true digit labels\n",
    "- Each row = a true digit (0-9)\n",
    "- Each column = a K-Means cluster (0-9)\n",
    "- Cell values = count of images with that true label assigned to that cluster\n",
    "- Ideally, each cluster would contain only one digit â†’ diagonal matrix\n",
    "\n",
    "**Adjusted Rand Index (ARI)**: A single number measuring clustering quality\n",
    "- Compares predicted clusters to true labels by counting pairwise agreements:\n",
    "  - Do two images from the same true class end up in the same cluster?\n",
    "  - Do two images from different true classes end up in different clusters?\n",
    "- **ARI = 1**: Perfect clustering (clusters exactly match true groups)\n",
    "- **ARI = 0**: No better than random chance\n",
    "- **ARI < 0**: Worse than random (rare)\n",
    "- Unlike simple accuracy, ARI is **adjusted for chance** â€” it accounts for the fact that random assignments can appear to agree by coincidence\n",
    "\n",
    "#### Math detour: ARI\n",
    "\n",
    "The **Adjusted Rand Index (ARI)** measures agreement between two clusterings, adjusting for chance:\n",
    "\n",
    "$$\\text{ARI} = \\frac{\\text{RI} - \\mathbb{E}[\\text{RI}]}{\\max(\\text{RI}) - \\mathbb{E}[\\text{RI}]}$$\n",
    "\n",
    "**The Rand Index (RI)** counts pairwise agreements:\n",
    "\n",
    "$$\\text{RI} = \\frac{a + b}{\\binom{n}{2}}$$\n",
    "\n",
    "Where:\n",
    "- $a$ = pairs of points in the **same** cluster in **both** clusterings (true positives)\n",
    "- $b$ = pairs of points in **different** clusters in **both** clusterings (true negatives)\n",
    "- $\\binom{n}{2}$ = total number of pairs\n",
    "\n",
    "**Intuition**: RI asks for each pair of points: \"Do the two clusterings agree on whether these points belong together?\"\n",
    "\n",
    "**Why adjust for chance?** With many clusters, random assignments can appear to agree by coincidence. The adjustment corrects for this expected agreement under random label assignment, which is why **ARI = 0 means \"no better than random\"** rather than \"no agreement at all.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prng = np.random.RandomState(20260225)\n",
    "\n",
    "sample_idx = prng.choice(len(X_mnist), sample_size, replace=False)\n",
    "X_sample = X_mnist[sample_idx]\n",
    "y_sample = y_mnist[sample_idx]\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=prng, n_init=10)\n",
    "labels = kmeans.fit_predict(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a8f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix between kmeans labels and true labels\n",
    "def plot_confusion_matrix(true_y, kmeans_labels):\n",
    "    conf_matrix = confusion_matrix(true_y, kmeans_labels)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix: KMeans Clusters vs True Labels\")\n",
    "    plt.xlabel(\"Predicted Clusters\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "\n",
    "plot_confusion_matrix(y_sample, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all digits classified as cluster 5 by KMeans\n",
    "def plot_cluster(cluster_label):\n",
    "    cluster_indices = np.where(labels == cluster_label)[0]\n",
    "    cluster_images = X_sample[cluster_indices]\n",
    "    cluster_true_labels = y_sample[cluster_indices]\n",
    "\n",
    "    n_images = len(cluster_indices)\n",
    "    n_cols = 10\n",
    "    n_rows = (n_images + n_cols - 1) // n_cols  # Ceiling division\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 1.2 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < n_images:\n",
    "            ax.imshow(cluster_images[i].reshape(28, 28), cmap='gray')\n",
    "            ax.set_title(f'{int(cluster_true_labels[i])}', fontsize=8)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.suptitle(f'Digits Classified as Cluster {cluster_label} by KMeans (n={n_images})\\nNumbers show true labels', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_cluster(cluster_label=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13be1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_cluster(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468405c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Express goodness of fit with ARI: adjusted rand index\n",
    "\n",
    "ari = adjusted_rand_score(y_sample, labels)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada2a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how K-Means performs with only 200 samples across different numbers of features\n",
    "\n",
    "prng = np.random.RandomState(20260226) # reset random state for reproducibility\n",
    "\n",
    "# Recreate sample\n",
    "sample_idx = prng.choice(len(X_mnist), sample_size, replace=False)\n",
    "X_sample = X_mnist[sample_idx]\n",
    "y_sample = y_mnist[sample_idx]\n",
    "\n",
    "# Test clustering quality across different numbers of randomly selected features\n",
    "feature_counts = [10, 25, 50, 100, 200, 400, 784]\n",
    "ari_scores = []\n",
    "\n",
    "for n_features in feature_counts:\n",
    "    # Randomly select features (simulating different dimensionalities)\n",
    "    selected_features = prng.choice(784, n_features, replace=False)\n",
    "    X_subset = X_sample[:, selected_features]\n",
    "    \n",
    "    # K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=10, random_state=prng, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_subset)\n",
    "    \n",
    "    # Measure quality\n",
    "    ari = adjusted_rand_score(y_sample, labels)\n",
    "    ari_scores.append(ari)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(feature_counts, ari_scores, 'o-', linewidth=2.5, markersize=10, color='navy')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5, label='Random clustering (ARI=0)')\n",
    "\n",
    "ax.set_xlabel('Number of Features (randomly selected)', fontsize=12)\n",
    "ax.set_ylabel('Adjusted Rand Index (clustering quality)', fontsize=12)\n",
    "ax.set_title(f'Clustering quality by the number of features used', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4ae93d",
   "metadata": {},
   "source": [
    "## Can PCA Help? Combating the Curse\n",
    "\n",
    "**Recall from Class 2**: PCA finds directions of maximum variance.\n",
    "\n",
    "**Key insight**: In MNIST, PCA automatically focuses on **informative pixels** (digit center) and ignores **uninformative pixels** (borders). By reducing dimensions, we:\n",
    "\n",
    "1. **Filter out noise**: Low-variance dimensions are discarded\n",
    "2. **Concentrate signal**: Remaining dimensions contain the useful information\n",
    "3. **Make distances meaningful again**: Distances now reflect actual digit similarity\n",
    "\n",
    "Let's see if PCA helps our clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc3b5b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T09:13:27.219744Z",
     "iopub.status.busy": "2026-02-23T09:13:27.219656Z",
     "iopub.status.idle": "2026-02-23T09:13:31.931088Z",
     "shell.execute_reply": "2026-02-23T09:13:31.930706Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare: Direct K-Means vs PCA + K-Means\n",
    "\n",
    "prng = np.random.RandomState(20260225) # reset random state for reproducibility\n",
    "\n",
    "# Recreate sample\n",
    "sample_idx = prng.choice(len(X_mnist), sample_size, replace=False)\n",
    "X_sample = X_mnist[sample_idx]\n",
    "y_sample = y_mnist[sample_idx]\n",
    "\n",
    "component_counts = [10, 25, 50, 100, 200]\n",
    "\n",
    "ari_scores_pca = []\n",
    "\n",
    "for n_component in component_counts:\n",
    "    pca = PCA(n_components=n_component, random_state=prng) # the standard optimizer uses randomization\n",
    "    X_pca = pca.fit_transform(X_sample)\n",
    "    kmeans_pca = KMeans(n_clusters=10, random_state=prng, n_init=10)\n",
    "    labels_pca = kmeans_pca.fit_predict(X_pca)\n",
    "    ari_pca = adjusted_rand_score(y_sample, labels_pca)\n",
    "    \n",
    "    ari_scores_pca.append(ari_pca)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(feature_counts, ari_scores, 'o-', linewidth=2.5, markersize=10, color='navy', label='Direct K-Means')\n",
    "ax.plot(component_counts, ari_scores_pca, 'o-', linewidth=2.5, markersize=10, color='darkred', label='PCA + K-Means')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5, label='Random clustering (ARI=0)')\n",
    "\n",
    "ax.set_xlabel('Number of Features / components', fontsize=12)\n",
    "ax.set_ylabel('Adjusted Rand Index (clustering quality)', fontsize=12)\n",
    "ax.set_title(f'Clustering quality by the number of features used and PCA', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba560ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**We've seen the problem**: High dimensions hurt clustering, especially with limited data. PCA can help by filtering noise.\n",
    "\n",
    "**But how does K-Means actually work?** Before we apply it to real-world problems, let's understand the algorithm itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44746dbe",
   "metadata": {},
   "source": [
    "## K-Means Algorithm: How Does It Work?\n",
    "\n",
    "Now that we've seen PCA helps clustering, let's understand **how K-Means actually works**.\n",
    "\n",
    "### The Intuition\n",
    "\n",
    "**K-Means** is one of the most popular clustering algorithms. It works through a simple iterative process:\n",
    "\n",
    "1. **Choose K** (the number of clusters you want to find)\n",
    "2. **Initialize**: Randomly place K \"cluster centers\" (centroids)\n",
    "3. **Repeat** until convergence:\n",
    "   - **Assignment step**: Assign each observation to the nearest centroid\n",
    "   - **Update step**: Move each centroid to the average position of its assigned observations\n",
    "\n",
    "**Goal**: Minimize the total distance between observations and their assigned centroids\n",
    "\n",
    "**Visual explanation**: [StatQuest video on K-Means](https://www.youtube.com/watch?v=4b5d3muPQmA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ec8ce",
   "metadata": {},
   "source": [
    "#### Math Detour: K-Means Objective Function\n",
    "\n",
    "K-Means minimizes the **within-cluster sum of squares (WCSS)**, also called **inertia**:\n",
    "\n",
    "$$\\text{WCSS} = \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} \\|x_i - \\mu_k\\|^2$$\n",
    "\n",
    "Where:\n",
    "- $K$ = number of clusters\n",
    "- $C_k$ = set of observations in cluster $k$\n",
    "- $\\mu_k$ = centroid (mean) of cluster $k$\n",
    "- $\\|x_i - \\mu_k\\|^2$ = squared Euclidean distance\n",
    "\n",
    "**Intuition**: Sum up all squared distances from each point to its assigned cluster center. Smaller is better!\n",
    "\n",
    "**Assignment step**: For each observation $x_i$, assign to cluster $k$ that minimizes $\\|x_i - \\mu_k\\|$\n",
    "\n",
    "**Update step**: For each cluster $k$, update centroid as the mean: $\\mu_k = \\frac{1}{|C_k|} \\sum_{x_i \\in C_k} x_i$\n",
    "\n",
    "These two steps alternate until centroids stop moving (convergence)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83f979c",
   "metadata": {},
   "source": [
    "### Choosing K: How Many Clusters?\n",
    "\n",
    "**Problem**: In real data, we usually don't know the true number of clusters!\n",
    "\n",
    "For MNIST we know K=10 (digits 0-9), but what if we didn't?\n",
    "\n",
    "**Solution**: Use the **Elbow Method** with inertia\n",
    "\n",
    "**Inertia** = Total squared distance from observations to their assigned centroids\n",
    "- **Lower is better** (observations closer to centroids)\n",
    "- But inertia always decreases as K increases!\n",
    "- Look for the \"elbow\" - where improvement slows down\n",
    "\n",
    "**Other metrics exist** (like Silhouette Score), but the elbow method is simple and usually sufficient. Use alternative metrics when the elbow is unclear. (Note that ARI only works if we know the true labels.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de51288",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T09:13:32.078562Z",
     "iopub.status.busy": "2026-02-23T09:13:32.078488Z",
     "iopub.status.idle": "2026-02-23T09:13:33.354302Z",
     "shell.execute_reply": "2026-02-23T09:13:33.353758Z"
    }
   },
   "outputs": [],
   "source": [
    "prng = np.random.RandomState(20260225) # reset random state for reproducibility\n",
    "\n",
    "# Recreate a larger sample\n",
    "sample_size = 1000\n",
    "sample_idx = prng.choice(len(X_mnist), sample_size, replace=False)\n",
    "X_sample = X_mnist[sample_idx]\n",
    "y_sample = y_mnist[sample_idx]\n",
    "\n",
    "pca = PCA(n_components=100, random_state=prng)\n",
    "X_pca = pca.fit_transform(X_sample)\n",
    "\n",
    "# Calculate inertia for K = 1 to 20\n",
    "K_range = range(1, 21)\n",
    "inertias = []\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=prng, n_init=10)\n",
    "    kmeans_temp.fit(X_pca)\n",
    "    inertias.append(kmeans_temp.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range, inertias, marker='o', linewidth=2, markersize=8)\n",
    "plt.axvline(x=10, color='red', linestyle='--', alpha=0.7, label='K=10 (true # of digits)')\n",
    "plt.xlabel('Number of Clusters (K)', fontsize=12)\n",
    "plt.ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12)\n",
    "plt.title('Elbow Method: Finding the optimal K where the curve bends', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e4369",
   "metadata": {},
   "source": [
    "### Why Not Use Supervised Learning?\n",
    "\n",
    "You might be thinking: **\"We have digit labels! Why not just train a classifier?\"**\n",
    "\n",
    "Great question! In MNIST, we used labels only to **evaluate** clustering quality (via ARI). But in practice:\n",
    "\n",
    "**Supervised learning requires labels** â†’ Expensive/impossible to obtain:\n",
    "- Medical diagnosis: Expert radiologists must label thousands of images\n",
    "- Customer segmentation: No \"correct\" groupings exist a priori\n",
    "- Anomaly detection: Anomalies are rare and unlabeled\n",
    "- Gene expression: Biological pathways aren't fully known\n",
    "\n",
    "**Clustering is for discovery** â†’ Find structure you didn't know existed:\n",
    "- What customer segments naturally exist?\n",
    "- Are there distinct subtypes of this disease?\n",
    "- Which countries form natural economic groupings?\n",
    "\n",
    "### Real-World Example: Country Segmentation\n",
    "\n",
    "In Class 2, we used PCA to understand economic indicators across countries. Now let's ask:\n",
    "\n",
    "**Can K-Means discover meaningful country groupings?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbd0b9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T09:13:33.572664Z",
     "iopub.status.busy": "2026-02-23T09:13:33.572579Z",
     "iopub.status.idle": "2026-02-23T09:13:33.576867Z",
     "shell.execute_reply": "2026-02-23T09:13:33.576418Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the World Bank economic indicators data (same as Class 2)\n",
    "df_countries = pd.read_csv('../data/economic_indicators/countries_wb_2020_2023.csv')\n",
    "\n",
    "# Define features for clustering (same 9 indicators from PCA class)\n",
    "feature_cols = ['GDP_per_capita', 'Inflation_rate', 'Unemployment_rate', \n",
    "                'Trade_percent_GDP', 'Health_spending_pct_GDP', 'Life_expectancy',\n",
    "                'Internet_users_pct', 'RD_spending_pct_GDP', 'FDI_pct_GDP']\n",
    "\n",
    "# Extract features\n",
    "X_countries = df_countries[feature_cols].copy()\n",
    "\n",
    "print(f\"Countries: {len(df_countries)}\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"\\nFeatures: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4521d6",
   "metadata": {},
   "source": [
    "**Recall from Class 2**: Features have vastly different scales (GDP in tens of thousands vs. percentages). We must **standardize** before clustering, just like we did for PCA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086de6b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T09:13:33.578386Z",
     "iopub.status.busy": "2026-02-23T09:13:33.578311Z",
     "iopub.status.idle": "2026-02-23T09:13:33.581375Z",
     "shell.execute_reply": "2026-02-23T09:13:33.580992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standardize features (critical for distance-based clustering!)\n",
    "scaler_countries = StandardScaler()\n",
    "X_countries_scaled = scaler_countries.fit_transform(X_countries)\n",
    "\n",
    "print(\"After standardization:\")\n",
    "print(f\"Means (should be ~0): {X_countries_scaled.mean(axis=0).round(2)}\")\n",
    "print(f\"Std devs (should be ~1): {X_countries_scaled.std(axis=0).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f61ad",
   "metadata": {},
   "source": [
    "### Optional Exercise\n",
    "\n",
    "Experiment with various numbers of clusters, whether to apply PCA or not, etc., and see what you can learn from the data. Unsupervised learning can be part of exploratory data analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd87b1f",
   "metadata": {},
   "source": [
    "## When K-Means Fails: Non-Spherical Clusters\n",
    "\n",
    "K-Means has a fundamental assumption: **clusters are spherical** (round in all dimensions).\n",
    "\n",
    "**What if our data has different shapes?**\n",
    "\n",
    "Let's look at a challenging dataset where K-Means will fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e30321",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T09:13:33.759923Z",
     "iopub.status.busy": "2026-02-23T09:13:33.759852Z",
     "iopub.status.idle": "2026-02-23T09:13:33.872127Z",
     "shell.execute_reply": "2026-02-23T09:13:33.871681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate moon-shaped clusters\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "\n",
    "# Try K-Means on this data\n",
    "kmeans_moons = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "kmeans_labels_moons = kmeans_moons.fit_predict(X_moons)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: True structure\n",
    "axes[0].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='viridis', alpha=0.6, s=50)\n",
    "axes[0].set_title('True Clusters\\n(Non-spherical moons)', fontsize=12)\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "\n",
    "# Right: K-Means result\n",
    "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], c=kmeans_labels_moons, cmap='viridis', alpha=0.6, s=50)\n",
    "axes[1].scatter(kmeans_moons.cluster_centers_[:, 0], kmeans_moons.cluster_centers_[:, 1],\n",
    "                c='red', marker='X', s=300, edgecolors='black', linewidth=2)\n",
    "axes[1].set_title('K-Means Clustering (K=2)\\nâŒ Fails on non-spherical clusters!', fontsize=12)\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ari_kmeans = adjusted_rand_score(y_moons, kmeans_labels_moons)\n",
    "print(f\"K-Means ARI: {ari_kmeans:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24785df2",
   "metadata": {},
   "source": [
    "### DBSCAN: Density-Based Clustering\n",
    "\n",
    "**Solution**: Use density-based clustering!\n",
    "\n",
    "**DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) identifies clusters based on **density**, not distance to centroids.\n",
    "\n",
    "**Core idea**: Clusters are dense regions separated by sparse regions\n",
    "\n",
    "DBSCAN identifies:\n",
    "1. **Core points**: Points with many neighbors within radius Îµ (epsilon)\n",
    "2. **Border points**: Points within Îµ of a core point (but not core themselves)\n",
    "3. **Noise points**: Points that are neither core nor border (outliers!)\n",
    "\n",
    "**Key parameters**:\n",
    "- **`eps` (Îµ)**: Maximum distance between two points to be neighbors\n",
    "- **`min_samples`**: Minimum number of neighbors to be a core point\n",
    "\n",
    "**Advantages over K-Means**:\n",
    "- Can find clusters of **arbitrary shape**\n",
    "- Automatically identifies **outliers**\n",
    "- **Don't need to specify number of clusters!**\n",
    "\n",
    "**Visual explanation**: [StatQuest video on DBSCAN](https://www.youtube.com/watch?v=RDZUdRSDOok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f7b7c9",
   "metadata": {},
   "source": [
    "#### Math Detour: DBSCAN Formal Definition\n",
    "\n",
    "DBSCAN uses two parameters to define density:\n",
    "- **Îµ (epsilon)**: Neighborhood radius\n",
    "- **MinPts**: Minimum points required to form a dense region\n",
    "\n",
    "**Definitions**:\n",
    "\n",
    "1. **Îµ-neighborhood** of point $p$: $N_\\varepsilon(p) = \\{q \\in D \\mid \\text{dist}(p,q) \\leq \\varepsilon\\}$\n",
    "   - All points within distance Îµ from point p\n",
    "\n",
    "2. **Core point**: A point $p$ is a core point if $|N_\\varepsilon(p)| \\geq \\text{MinPts}$\n",
    "   - Has at least MinPts neighbors (including itself) within radius Îµ\n",
    "\n",
    "3. **Directly density-reachable**: Point $q$ is directly density-reachable from $p$ if:\n",
    "   - $p$ is a core point, AND\n",
    "   - $q \\in N_\\varepsilon(p)$\n",
    "\n",
    "4. **Density-reachable**: Point $q$ is density-reachable from $p$ if there's a chain of directly density-reachable points connecting them\n",
    "\n",
    "5. **Density-connected**: Points $p$ and $q$ are density-connected if both are density-reachable from some core point $o$\n",
    "\n",
    "**Cluster formation**: A cluster is a maximal set of density-connected points. Points that aren't density-reachable from any core point are labeled as **noise** (-1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d026501f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T09:13:33.873336Z",
     "iopub.status.busy": "2026-02-23T09:13:33.873261Z",
     "iopub.status.idle": "2026-02-23T09:13:33.877138Z",
     "shell.execute_reply": "2026-02-23T09:13:33.876751Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply DBSCAN to moon data\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X_moons)\n",
    "\n",
    "# Count results\n",
    "n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_outliers = np.sum(dbscan_labels == -1)\n",
    "\n",
    "print(f\"Number of clusters found: {n_clusters}\")\n",
    "print(f\"Number of outliers: {n_outliers}\")\n",
    "\n",
    "ari_dbscan = adjusted_rand_score(y_moons, dbscan_labels)\n",
    "print(f\"DBSCAN ARI: {ari_dbscan:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528b1072",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T09:13:33.878257Z",
     "iopub.status.busy": "2026-02-23T09:13:33.878185Z",
     "iopub.status.idle": "2026-02-23T09:13:34.029612Z",
     "shell.execute_reply": "2026-02-23T09:13:34.029213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize comparison: K-Means vs DBSCAN\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Left: True clusters\n",
    "axes[0].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='viridis', alpha=0.6, s=50)\n",
    "axes[0].set_title('True Clusters', fontsize=12)\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "\n",
    "# Middle: K-Means (failed)\n",
    "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], c=kmeans_labels_moons, cmap='viridis', alpha=0.6, s=50)\n",
    "axes[1].scatter(kmeans_moons.cluster_centers_[:, 0], kmeans_moons.cluster_centers_[:, 1],\n",
    "                c='red', marker='X', s=300, edgecolors='black', linewidth=2)\n",
    "axes[1].set_title(f'K-Means (K=2)\\n ARI = {ari_kmeans:.3f}', fontsize=12)\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "\n",
    "# Right: DBSCAN (success!)\n",
    "# Plot clusters\n",
    "mask_clustered = dbscan_labels != -1\n",
    "axes[2].scatter(X_moons[mask_clustered, 0], X_moons[mask_clustered, 1], \n",
    "                c=dbscan_labels[mask_clustered], cmap='viridis', alpha=0.6, s=50)\n",
    "# Plot outliers in black\n",
    "if n_outliers > 0:\n",
    "    mask_outliers = dbscan_labels == -1\n",
    "    axes[2].scatter(X_moons[mask_outliers, 0], X_moons[mask_outliers, 1],\n",
    "                    c='black', marker='x', s=100, alpha=0.5, label='Outliers')\n",
    "    axes[2].legend()\n",
    "axes[2].set_title(f'DBSCAN (eps={dbscan.eps}, min_samples={dbscan.min_samples})\\n ARI = {ari_dbscan:.3f}', fontsize=12)\n",
    "axes[2].set_xlabel('Feature 1')\n",
    "axes[2].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ¯ DBSCAN successfully finds the crescent-shaped clusters!\")\n",
    "print(\"   It works by connecting dense regions, not finding centroids.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6547580",
   "metadata": {},
   "source": [
    "### Choosing DBSCAN Parameters\n",
    "\n",
    "**Challenge**: How to choose `eps` and `min_samples`?\n",
    "\n",
    "**Rules of thumb**:\n",
    "- **`min_samples`**: \n",
    "  - Start with 2 Ã— dimensions + 1 (e.g., 5 for 2D data)\n",
    "  - Larger values = fewer, denser clusters\n",
    "- **`eps`**: \n",
    "  - Look at k-distance graph (distance to k-th nearest neighbor)\n",
    "  - Choose eps at the \"elbow\" of this graph\n",
    "  - Or use domain knowledge about meaningful distance\n",
    "\n",
    "Let's see how different `eps` values affect the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e28341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T09:13:34.031566Z",
     "iopub.status.busy": "2026-02-23T09:13:34.031474Z",
     "iopub.status.idle": "2026-02-23T09:13:34.260119Z",
     "shell.execute_reply": "2026-02-23T09:13:34.259464Z"
    }
   },
   "outputs": [],
   "source": [
    "# Experiment with different eps values\n",
    "eps_values = [0.05, 0.1, 0.2, 0.5]\n",
    "fig, axes = plt.subplots(1, len(eps_values), figsize=(18, 4))\n",
    "\n",
    "for idx, eps_val in enumerate(eps_values):\n",
    "    dbscan_temp = DBSCAN(eps=eps_val, min_samples=5)\n",
    "    labels_temp = dbscan_temp.fit_predict(X_moons)\n",
    "    \n",
    "    n_clusters_temp = len(set(labels_temp)) - (1 if -1 in labels_temp else 0)\n",
    "    n_outliers_temp = np.sum(labels_temp == -1)\n",
    "    \n",
    "    # Plot\n",
    "    mask_clustered = labels_temp != -1\n",
    "    if np.any(mask_clustered):\n",
    "        axes[idx].scatter(X_moons[mask_clustered, 0], X_moons[mask_clustered, 1],\n",
    "                         c=labels_temp[mask_clustered], cmap='viridis', alpha=0.6, s=50)\n",
    "    if n_outliers_temp > 0:\n",
    "        mask_outliers = labels_temp == -1\n",
    "        axes[idx].scatter(X_moons[mask_outliers, 0], X_moons[mask_outliers, 1],\n",
    "                         c='black', marker='x', s=100, alpha=0.5)\n",
    "    \n",
    "    axes[idx].set_title(f'eps={eps_val}\\nClusters: {n_clusters_temp}, Outliers: {n_outliers_temp}', fontsize=11)\n",
    "    axes[idx].set_xlabel('Feature 1')\n",
    "    axes[idx].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95687e2d",
   "metadata": {},
   "source": [
    "**Observation**: eps controls the neighborhood size\n",
    "- Too small (0.05): Everything becomes noise\n",
    "- Too large (0.5): All points merge into one cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ed9d2",
   "metadata": {},
   "source": [
    "### K-Means vs DBSCAN: When to Use Which?\n",
    "\n",
    "| Feature | K-Means | DBSCAN |\n",
    "|---------|---------|--------|\n",
    "| **Cluster shape** | Spherical only | Any shape |\n",
    "| **Number of clusters** | Must specify K | Automatic |\n",
    "| **Handles outliers** | No (assigns all points) | Yes (labels as -1) |\n",
    "| **Cluster sizes** | Similar sizes work best | Can vary |\n",
    "| **Speed** | Very fast O(nÃ—KÃ—dÃ—i) | Slower O(nÂ²) or O(n log n) |\n",
    "| **Parameters** | K (# clusters) | eps, min_samples |\n",
    "\n",
    "**Use K-Means when**:\n",
    "- You know (or can estimate) the number of clusters\n",
    "- Clusters are roughly spherical and similar in size\n",
    "- Speed is important (K-Means is fast!)\n",
    "\n",
    "**Use DBSCAN when**:\n",
    "- Clusters have irregular shapes\n",
    "- You want to identify outliers\n",
    "- You don't know the number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f306376",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key takeaways\n",
    "\n",
    "1. **Clustering is unsupervised**: We find patterns without labels, making evaluation inherently challenging\n",
    "2. **K-Means is simple but limited**: Fast and intuitive, but assumes spherical clusters and requires specifying K\n",
    "3. **Choosing K matters**: Use the elbow method on an appropriate metric (like inertia), but remember this is heuristic\n",
    "4. **Curse of dimensionality affects clustering**: With many features, distances become less meaningful -> consider PCA first\n",
    "5. **DBSCAN handles complexity**: Discovers arbitrary shapes and identifies outliers, but requires tuning eps and min_samples\n",
    "6. **No universal best algorithm**: Choose based on your data structure, domain knowledge, and what you want to discover\n",
    "7. **Preprocessing is crucial**: Always scale features before clustering (and before PCA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
