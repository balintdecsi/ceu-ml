{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99e18ba5",
   "metadata": {},
   "source": [
    "# Class 2: Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f8266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 110\n",
    "%precision 3\n",
    "pd.set_option('display.precision', 3)\n",
    "np.set_printoptions(suppress=True, precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19ae1d3",
   "metadata": {},
   "source": [
    "## Motivating PCA: from 2 features to 1\n",
    "\n",
    "**The central question:** Can we summarize information from multiple features using fewer features?\n",
    "\n",
    "**Why reduce dimensions?**\n",
    "- Combat curse of dimensionality: high dimensions make similarity less meaningful\n",
    "- Visualization: easier to see patterns in 2D than in 9D\n",
    "- Understanding: discover the \"main stories\" hidden in many variables\n",
    "- Computation: fewer features = faster algorithms\n",
    "\n",
    "Let's work with country-level economic data from the World Bank to assess the development of countries. First, we'll focus on two important indicators that form the **Phillips curve**: the empirical relationship between unemployment and inflation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2af2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('../data/economic_indicators/countries_wb_2020_2023.csv')\n",
    "\n",
    "# EU countries & relevant features\n",
    "wb_eu = df[df['is_EU'] == 1][['Unemployment_rate', 'Inflation_rate']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f965fac",
   "metadata": {},
   "source": [
    "### Recall: what is the first step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b25bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Know your data - always start by exploring it\n",
    "wb_eu.describe()\n",
    "\n",
    "# already nicely put together for you to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553589d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 6))\n",
    "plt.scatter(wb_eu['Unemployment_rate'], wb_eu['Inflation_rate'], alpha=0.7, s=50)\n",
    "plt.xlabel('Unemployment rate (%)')\n",
    "plt.ylabel('Inflation rate (%)')\n",
    "plt.title('Phillips Curve: EU Countries (2020-2023)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlim(0, 18)\n",
    "plt.ylim(0, 18)\n",
    "\n",
    "# Create all text annotations\n",
    "texts = []\n",
    "for idx, country in enumerate(df[df['is_EU'] == 1]['Country']):\n",
    "    texts.append(plt.text(\n",
    "        wb_eu['Unemployment_rate'].iloc[idx], \n",
    "        wb_eu['Inflation_rate'].iloc[idx], \n",
    "        country, \n",
    "        fontsize=8, alpha=0.7\n",
    "    ))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913d53a1",
   "metadata": {},
   "source": [
    "### Projection: The Key Idea\n",
    "\n",
    "**The question:** If we must compress these two features into one, how should we do it?\n",
    "\n",
    "**The answer:** Project the data onto a line. But which line?\n",
    "\n",
    "**Intuition:** When we project points onto a line, we're \"flattening\" our 2D data into 1D. Different lines preserve different amounts of information:\n",
    "- If projected points are **spread far apart**, we preserve more information (we can still distinguish countries)\n",
    "- If projected points **cluster together**, we lose information (countries become indistinguishable)\n",
    "\n",
    "**Measuring information:** We quantify \"spread\" using **variance**. Higher variance along the projection direction = more information preserved.\n",
    "\n",
    "**PCA's goal:** Find the line (direction) that maximizes variance when we project onto it.\n",
    "\n",
    "Let's visualize this by projecting our European countries data onto several different directions and comparing how much variance each preserves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b57e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_with_projection(direction_vector, data, ax=None):\n",
    "    \"\"\"Plot data points and their projections onto a given direction through the data center.\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    direction = direction_vector / np.linalg.norm(direction_vector)  # normalize to unit vector\n",
    "    \n",
    "    # Get the data as array\n",
    "    X_array = data.values\n",
    "    \n",
    "    # Calculate the center point (mean)\n",
    "    center = X_array.mean(axis=0)\n",
    "    \n",
    "    # Center the data\n",
    "    X_centered = X_array - center\n",
    "    \n",
    "    # Plot original data points\n",
    "    ax.scatter(X_array[:, 0], X_array[:, 1], alpha=0.6, s=50, label='Original points', zorder=3)\n",
    "\n",
    "    # Calculate projections onto the direction (for centered data)\n",
    "    projections_1d = X_centered @ direction  # scalar projection for each point\n",
    "    projections_centered = np.outer(projections_1d, direction)  # back to 2D coordinates (centered)\n",
    "    projections = projections_centered + center  # shift back to original coordinate system\n",
    "\n",
    "    # Plot projection points\n",
    "    ax.scatter(projections[:, 0], projections[:, 1], color='darkred', alpha=0.6, s=50, \n",
    "               label='Projected points', zorder=3)\n",
    "\n",
    "    # Draw lines between original points and their projections\n",
    "    for i in range(len(X_array)):\n",
    "        ax.plot([X_array[i, 0], projections[i, 0]], [X_array[i, 1], projections[i, 1]], \n",
    "                color='gray', alpha=0.3, linewidth=1, zorder=1)\n",
    "\n",
    "    # Draw the projection direction as a line through the center point\n",
    "    scale = 15\n",
    "    ax.plot([center[0] - scale * direction[0], center[0] + scale * direction[0]], \n",
    "            [center[1] - scale * direction[1], center[1] + scale * direction[1]], \n",
    "            color='darkred', linewidth=2, alpha=0.7, label='Projection direction', zorder=2)\n",
    "\n",
    "    ax.set_xlabel('Unemployment rate (%)')\n",
    "    ax.set_ylabel('Inflation rate (%)')\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    ax.set_xlim(0, 18)\n",
    "    ax.set_ylim(0, 18)\n",
    "\n",
    "    # Calculate and display variance preserved\n",
    "    variance_preserved = np.var(projections_1d)\n",
    "    ax.text(0.05, 0.95, f'Variance: {variance_preserved:.2f}', \n",
    "            transform=ax.transAxes, verticalalignment='top', \n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226c3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2x2 grid of plots with different projection directions\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 12), sharex=True, sharey=True)\n",
    "\n",
    "# Try different projection directions\n",
    "plot_data_with_projection([1, 0], wb_eu, ax=ax1)\n",
    "ax1.set_title(\"Projection onto horizontal\")\n",
    "\n",
    "plot_data_with_projection([0, 1], wb_eu, ax=ax2)\n",
    "ax2.set_title(\"Projection onto vertical\")\n",
    "\n",
    "plot_data_with_projection([1, 1], wb_eu, ax=ax3)\n",
    "ax3.set_title(\"Projection onto diagonal\")\n",
    "\n",
    "plot_data_with_projection([-1, 1], wb_eu, ax=ax4)\n",
    "ax4.set_title(\"Projection onto anti-diagonal\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e9b9f8",
   "metadata": {},
   "source": [
    "**What do we observe?**\n",
    "\n",
    "**Key insight:** Some directions capture more of the data's natural spread than others. The direction that maximizes variance is the **first principal component (PC1)**. Higher variance means that points are spread out more, allowing us to better distinguish them.\n",
    "\n",
    "Note that higher variance also means lower projection error (the distance between the original blue points and their projected red counterparts).\n",
    "\n",
    "For a nice illustration, check this answer on [Cross Validated](https://stats.stackexchange.com/a/140579).\n",
    "\n",
    "**PCA's algorithm:**\n",
    "1. Find the direction with maximum variance → **PC1**\n",
    "2. Find the perpendicular direction with maximum remaining variance → **PC2**\n",
    "3. Continue for higher dimensions..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00623bd4",
   "metadata": {},
   "source": [
    "### Applying PCA with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d167c198",
   "metadata": {},
   "source": [
    "#### Technical detour: sklearn model syntax\n",
    "\n",
    "Most sklearn models follow a consistent workflow:\n",
    "1. **Define** the model object (with parameters)\n",
    "2. **Fit** the model to data (learn from the data)\n",
    "3. **Transform** new data using the fitted model\n",
    "\n",
    "Some steps can be merged (e.g., `fit_transform()`) or reversed (e.g., `inverse_transform()`).\n",
    "\n",
    "Let's see this in action with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d0e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA and calculate the principal components as X_pca\n",
    "pca = PCA() #definition of the model\n",
    "pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c093b242",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_ # no components yet, the model is just defined, not fitted yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a2bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(wb_eu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e200363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f8c62",
   "metadata": {},
   "source": [
    "Now we have the principal components! These are the directions PCA found. \n",
    "The `components_` attribute shows the **loadings**: how each original feature contributes to each principal component.\n",
    "\n",
    "*Unlike many ML algorithms, PCA is deterministic: you'll get the same results every time. \n",
    "The only ambiguity is the **sign** of components, which can be flipped without changing the meaning.\n",
    "This is why we did not have to set the seed for reproducibility.*\n",
    "\n",
    "Now let's **transform** the data to get the coordinates in the new PC space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897eec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data using PCA\n",
    "wb_eu_pca = pca.transform(wb_eu)\n",
    "\n",
    "# Note: You can also use fit_transform() to fit and transform in one step:\n",
    "# wb_eu_pca = pca.fit_transform(wb_eu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea5cb35",
   "metadata": {},
   "source": [
    "#### Technical detour: DataFrame vs Array\n",
    "\n",
    "The result is a numpy array (not a DataFrame), containing each country's coordinates in PC space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf9de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type of the original data: \", type(wb_eu))\n",
    "print(\"Type of the transformed data: \", type(wb_eu_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcaf3c3",
   "metadata": {},
   "source": [
    "### Visualizing the PCA transformation\n",
    "\n",
    "**What PCA does geometrically:**\n",
    "1. **Centers the data** at the origin (shifts the mean to 0, 0)\n",
    "2. **Finds PC1**: the direction of maximum variance through the centered data\n",
    "3. **Finds PC2**: perpendicular to PC1, capturing the maximum remaining variance\n",
    "4. **Rotates the coordinate system**: aligns axes with these principal component directions\n",
    "\n",
    "Let's visualize this transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6b95d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the original data and the principal components\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 16))\n",
    "    \n",
    "fig.suptitle('Transform Data into Principal Component Space', fontsize=16)\n",
    "\n",
    "# Subplot 1: Original data with PC directions\n",
    "X_array = wb_eu.values\n",
    "center = X_array.mean(axis=0)\n",
    "\n",
    "ax1.scatter(wb_eu[\"Unemployment_rate\"], wb_eu[\"Inflation_rate\"], alpha=0.6, s=50)\n",
    "ax1.set_xlabel('Unemployment rate (%)')\n",
    "ax1.set_ylabel('Inflation rate (%)')\n",
    "ax1.set_title('Original Feature Space')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Draw PC directions from the center point (where PCA centers the data)\n",
    "scale = 3  # scale the arrows for visibility\n",
    "ax1.arrow(center[0], center[1], scale * pca.components_[0, 0], scale * pca.components_[0, 1], \n",
    "          color='darkred', width=0.15, head_width=0.5, head_length=0.3, label='PC1', zorder=5)\n",
    "ax1.arrow(center[0], center[1], scale * pca.components_[1, 0], scale * pca.components_[1, 1], \n",
    "          color='darkgreen', width=0.15, head_width=0.5, head_length=0.3, label='PC2', zorder=5)\n",
    "# Draw the opposite directions (without labels to avoid duplicate legend entries)\n",
    "ax1.arrow(center[0], center[1], scale * -pca.components_[0, 0], scale * -pca.components_[0, 1], \n",
    "          color='darkred', width=0.15, head_width=0.5, head_length=0.3, zorder=5, alpha=0.3)\n",
    "ax1.arrow(center[0], center[1], scale * -pca.components_[1, 0], scale * -pca.components_[1, 1], \n",
    "          color='darkgreen', width=0.15, head_width=0.5, head_length=0.3, zorder=5, alpha=0.3)\n",
    "\n",
    "ax1.legend()\n",
    "ax1.set_xlim(0, 18)\n",
    "ax1.set_ylim(0, 18)\n",
    "\n",
    "# Subplot 2: Transformed data in PC space\n",
    "ax2.scatter(wb_eu_pca[:, 0], wb_eu_pca[:, 1], color='purple', alpha=0.6, s=50)\n",
    "ax2.set_xlabel('Principal Component 1')\n",
    "ax2.set_ylabel('Principal Component 2')\n",
    "ax2.set_title('Principal Component Space (Rotated & Centered)')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Draw PC axes as reference lines (without labels)\n",
    "ax2.axhline(y=0, color='darkred', linestyle='-', alpha=0.4, linewidth=2)\n",
    "ax2.axvline(x=0, color='darkgreen', linestyle='-', alpha=0.4, linewidth=2)\n",
    "\n",
    "ax2.set_aspect('equal', adjustable='box')  # Make the aspect ratio equal to show true rotation\n",
    "\n",
    "# Subplot 3: Projection onto PC1 in PC space\n",
    "# Get PC1 direction\n",
    "direction = pca.components_[0] / np.linalg.norm(pca.components_[0])\n",
    "\n",
    "# Calculate center and projections\n",
    "X_centered = X_array - center\n",
    "projections_1d = X_centered @ direction\n",
    "\n",
    "# Plot all points in PC space (from subplot 2)\n",
    "ax3.scatter(wb_eu_pca[:, 0], wb_eu_pca[:, 1], color='purple', alpha=0.6, s=50, \n",
    "           label='Transformed points', zorder=3)\n",
    "\n",
    "# Plot projected points (PC1 only, PC2 = 0)\n",
    "ax3.scatter(projections_1d, np.zeros_like(projections_1d), color='darkred', alpha=0.6, s=50, \n",
    "           label='Projected onto PC1', zorder=3)\n",
    "\n",
    "# Draw lines from transformed points to their projections on PC1 axis\n",
    "for i in range(len(wb_eu_pca)):\n",
    "    ax3.plot([wb_eu_pca[i, 0], projections_1d[i]], \n",
    "            [wb_eu_pca[i, 1], 0], \n",
    "            color='gray', alpha=0.3, linewidth=1, zorder=1)\n",
    "\n",
    "# Draw PC1 axis (horizontal) and PC2 axis (vertical)\n",
    "ax3.axhline(y=0, color='darkred', linestyle='-', alpha=0.7, linewidth=2, label='PC1 axis')\n",
    "ax3.axvline(x=0, color='darkgreen', linestyle='-', alpha=0.4, linewidth=2)\n",
    "\n",
    "ax3.set_xlabel('Principal Component 1')\n",
    "ax3.set_ylabel('Principal Component 2')\n",
    "ax3.set_title(f'Projection onto PC1 in PC Space ({pca.explained_variance_ratio_[0]*100:.1f}% of variance)')\n",
    "ax3.legend(loc='upper right', fontsize=10)\n",
    "ax3.grid(alpha=0.3)\n",
    "ax3.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Subplot 4: Projection onto First Principal Component (PC1)\n",
    "plot_data_with_projection(pca.components_[0], wb_eu, ax=ax4)\n",
    "ax4.set_title('Projection onto First Principal Component (PC1)')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a054253c",
   "metadata": {},
   "source": [
    "## Characteristics of PCA\n",
    "\n",
    "Now that we've seen PCA in action, let's understand its key properties:\n",
    "1. **Explained variance** - how much information each PC captures\n",
    "2. **Loadings** - how original features combine to form PCs\n",
    "3. **Orthogonality** - why PCs are perpendicular\n",
    "4. **Reconstruction error** - the cost of dimensionality reduction\n",
    "\n",
    "### 1. Explained Variance: quantifying information preserved\n",
    "\n",
    "PCA finds directions in order of how much variance they explain. Let's see how much of the total variance each PC captures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bae311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total variance in the original data (sum of variances of both features)\n",
    "total_variance = np.var(wb_eu, axis=0).sum()\n",
    "print(f'Total variance in original data: {total_variance:.2f}')\n",
    "total_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d916378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance explained by each principal component (in absolute terms)\n",
    "print('Variance explained by each PC:')\n",
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e421a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of total variance explained by each PC\n",
    "print('Proportion of variance explained by each PC:')\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4bf491",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'PC1 explains {pca.explained_variance_ratio_[0] * 100:.1f}% of total variance')\n",
    "print(f'PC2 explains {pca.explained_variance_ratio_[1] * 100:.1f}% of total variance')\n",
    "print(f'Together: {pca.explained_variance_ratio_.sum() * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746e233d",
   "metadata": {},
   "source": [
    "### 2. Loadings: what PCs represent\n",
    "\n",
    "**Loadings** tell us how each original feature contributes to each principal component.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67742882",
   "metadata": {},
   "source": [
    "#### Math detour\n",
    "\n",
    "For each country, the new PC coordinates are computed as linear combinations of the original (centered) features:\n",
    "\n",
    "$$\\text{PC1}_i = w_{1,1} \\cdot (\\text{unemployment}_i - \\text{avg}(\\text{unemployment})) + w_{1,2} \\cdot (\\text{inflation}_i - \\text{avg}(\\text{inflation}))$$\n",
    "\n",
    "$$\\text{PC2}_i = w_{2,1} \\cdot (\\text{unemployment}_i - \\text{avg}(\\text{unemployment})) + w_{2,2} \\cdot (\\text{inflation}_i - \\text{avg}({\\text{inflation}}))$$\n",
    "\n",
    "where:\n",
    "- $w_{j,k}$ are the loadings from `pca.components_` (row $j$, column $k$)\n",
    "- The centered features are the deviations from their means\n",
    "\n",
    "In matrix form: $\\mathbf{Z} = (\\mathbf{X} - \\bar{\\mathbf{X}}) \\mathbf{W}^T$, where $\\mathbf{W}$ is the matrix of principal component directions (loadings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334a88db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display loadings\n",
    "print(\"Loadings (how original features contribute to PCs):\\n\")\n",
    "loadings_df = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=['Unemployment_rate', 'Inflation_rate']\n",
    ")\n",
    "loadings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7035bed8",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- **PC1** contrasts the features (negative loading on unemployment, positive on inflation) → it represents the **trade-off** between unemployment and inflation. Countries with high PC1 scores have high inflation and low unemployment.\n",
    "- **PC2** has similar (positive) loadings on both features → it captures \"overall economic stress\" (high unemployment AND high inflation together)\n",
    "\n",
    "**Key property:** Loadings are the coordinates of the principal component directions. They form unit vectors (length = 1), which you can verify: $\\sqrt{\\text{loading}_1^2 + \\text{loading}_2^2} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d2b705",
   "metadata": {},
   "source": [
    "#### Manual calculation example\n",
    "\n",
    "Let's verify how to compute PC coordinates from loadings for a specific country (Austria):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the original values for the first countr (Austria)\n",
    "austria_data = wb_eu.iloc[0]\n",
    "print(\"Austria's original features:\")\n",
    "print(austria_data)\n",
    "print()\n",
    "\n",
    "# Step 2: Calculate the mean of each feature (what PCA uses for centering)\n",
    "feature_means = wb_eu.mean()\n",
    "print(\"Feature means (used for centering):\")\n",
    "print(feature_means)\n",
    "print()\n",
    "\n",
    "# Step 3: Calculate the PC1 and PC2 values manually using the PCA components and the centered data\n",
    "pc1_manual = (pca.components_[0, 0] * (austria_data['Unemployment_rate'] - feature_means['Unemployment_rate']) + \n",
    "              pca.components_[0, 1] * (austria_data['Inflation_rate'] - feature_means['Inflation_rate']))\n",
    "\n",
    "pc2_manual = (pca.components_[1, 0] * (austria_data['Unemployment_rate'] - feature_means['Unemployment_rate']) + \n",
    "              pca.components_[1, 1] * (austria_data['Inflation_rate'] - feature_means['Inflation_rate']))\n",
    "print(f\"Manual calculation:\")\n",
    "print(f\"PC1 = {pca.components_[0, 0]:.3f} × ({austria_data['Unemployment_rate']:.3f} - {feature_means['Unemployment_rate']:.3f}) + {pca.components_[0, 1]:.3f} × ({austria_data['Inflation_rate']:.3f} - {feature_means['Inflation_rate']:.3f}) = {pc1_manual:.3f}\")\n",
    "print(f\"PC2 = {pca.components_[1, 0]:.3f} × ({austria_data['Unemployment_rate']:.3f} - {feature_means['Unemployment_rate']:.3f}) + {pca.components_[1, 1]:.3f} × ({austria_data['Inflation_rate']:.3f} - {feature_means['Inflation_rate']:.3f}) = {pc2_manual:.3f}\")\n",
    "print()\n",
    "\n",
    "# Step 4: Compare with sklearn's transform\n",
    "print(f\"sklearn's transform result for Austria:\")\n",
    "print(f\"PC1 = {wb_eu_pca[0, 0]:.3f}\")\n",
    "print(f\"PC2 = {wb_eu_pca[0, 1]:.3f}\")\n",
    "print()\n",
    "print(\"✓ They match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb4026c",
   "metadata": {},
   "source": [
    "### 3. Orthogonality: why PCs are perpendicular\n",
    "\n",
    "**Observation:** In our visualizations, PC1 and PC2 are always perpendicular (orthogonal). This isn't a coincidence—it's by design.\n",
    "\n",
    "**Why orthogonality?** PCA's sequential construction ensures it:\n",
    "\n",
    "1. **PC1** finds the direction that maximizes variance (no constraints)\n",
    "2. **PC2** finds the direction that maximizes variance **among all directions perpendicular to PC1**\n",
    "3. **PC3** maximizes variance perpendicular to both PC1 and PC2, and so on...\n",
    "\n",
    "This sequential process has a beautiful consequence: each PC captures **independent** information (orthogonal = uncorrelated). You can verify this mathematically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify orthogonality: dot product of PC1 and PC2 directions should be zero\n",
    "dot_product = pca.components_[0] @ pca.components_[1]\n",
    "print(f'Dot product of PC1 and PC2: {dot_product:.10f}')\n",
    "print('(Should be essentially zero, confirming orthogonality)')\n",
    "\n",
    "# Also verify: correlation between PC scores should be zero\n",
    "correlation = np.corrcoef(wb_eu_pca[:, 0], wb_eu_pca[:, 1])[0, 1]\n",
    "print(f'\\nCorrelation between PC1 and PC2 scores: {correlation:.10f}')\n",
    "print('(Zero correlation = independent dimensions)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf7e9c",
   "metadata": {},
   "source": [
    "#### Math detour: Why are PCs orthogonal?\n",
    "\n",
    "The orthogonality of principal components comes from the **eigendecomposition of the covariance matrix**. \n",
    "\n",
    "**The key mathematical fact:** When we compute PCA, we're finding the eigenvectors of the data's covariance matrix. Since covariance matrices are symmetric, their eigenvectors are guaranteed to be orthogonal (perpendicular) to each other.\n",
    "\n",
    "**The intuition:** \n",
    "- The covariance matrix captures how features vary together\n",
    "- Its eigenvectors point in the directions of maximum variance\n",
    "- The eigenvalues tell us how much variance is in each direction\n",
    "- PCA sorts these by eigenvalue (largest first) to get PC1, PC2, PC3, ...\n",
    "\n",
    "For an excellent intuitive explanation of how \"maximizing variance\" is equivalent to \"finding eigenvectors of the covariance matrix,\" see [this answer on Cross Validated](https://stats.stackexchange.com/a/219344)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea9e6e8",
   "metadata": {},
   "source": [
    "### 4. Reconstruction Error: the cost of dimensionality reduction\n",
    "\n",
    "**The trade-off:** If we use only PC1 (reducing from 2D to 1D), we lose some information. How much?\n",
    "\n",
    "**Reconstruction:** We can \"reverse\" PCA to go back from PC space to the original feature space. This reconstructed data won't be perfect: the difference is the **reconstruction error**.\n",
    "\n",
    "**Key relationship:** \n",
    "- Variance explained by PCs = Information preserved\n",
    "- Reconstruction error = Information lost\n",
    "- They sum to 100% of the original variance\n",
    "\n",
    "Let's reconstruct our data using only PC1 and measure the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110224ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA with only 1 component\n",
    "pca1 = PCA(n_components=1)\n",
    "feature_1d = pca1.fit_transform(wb_eu)\n",
    "print(f'Reduced data shape: {feature_1d.shape} -- from {wb_eu.shape}')\n",
    "feature_1d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753dcb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct back to 2D using inverse_transform\n",
    "reconstructed_data = pca1.inverse_transform(feature_1d)\n",
    "print(f'Reconstructed data shape: {reconstructed_data.shape}')\n",
    "reconstructed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f208ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original vs reconstructed data\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.scatter(wb_eu[\"Unemployment_rate\"], wb_eu[\"Inflation_rate\"], \n",
    "           alpha=0.6, s=80, label=\"Original\", color='steelblue')\n",
    "plt.scatter(reconstructed_data[:, 0], reconstructed_data[:, 1], \n",
    "           alpha=0.6, s=80, color=\"darkred\", label=\"Reconstructed (from PC1 only)\")\n",
    "\n",
    "# Draw lines showing reconstruction error for each point\n",
    "for i in range(len(wb_eu)):\n",
    "    plt.plot([wb_eu[\"Unemployment_rate\"].iloc[i], reconstructed_data[i, 0]], \n",
    "            [wb_eu[\"Inflation_rate\"].iloc[i], reconstructed_data[i, 1]], \n",
    "            color='gray', alpha=0.3, linewidth=1)\n",
    "\n",
    "plt.xlabel(\"Unemployment rate (%)\")\n",
    "plt.ylabel(\"Inflation rate (%)\")\n",
    "plt.title(\"Original vs Reconstructed: Information Loss from Using Only PC1\")\n",
    "plt.legend()\n",
    "plt.xlim(0, 18)\n",
    "plt.ylim(0, 18)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1dee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reconstruction error (mean squared distance between original and reconstructed)\n",
    "def calculateReconstructionError(X, X_reconstructed):\n",
    "    \"\"\"Calculate average squared distance = variance not explained\"\"\"\n",
    "    return np.mean(np.sum(np.square(X.values - X_reconstructed), axis=1))\n",
    "\n",
    "reconstruction_error = calculateReconstructionError(wb_eu, reconstructed_data)\n",
    "print(f'Reconstruction error: {reconstruction_error:.2f}')\n",
    "reconstruction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa59dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the relationship: explained variance + reconstruction error = total variance\n",
    "relative_error = reconstruction_error / total_variance\n",
    "print(f'Relative reconstruction error: {relative_error * 100:.1f}%')\n",
    "print(f'Variance explained by PC1: {pca1.explained_variance_ratio_[0] * 100:.1f}%')\n",
    "print(f'Sum: {(relative_error + pca1.explained_variance_ratio_[0]) * 100:.1f}%')\n",
    "print('\\n✓ They sum to 100%: reconstruction error measures what we lost!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6929d1",
   "metadata": {},
   "source": [
    "### Connection to OLS regression\n",
    "\n",
    "You've seen OLS (Ordinary Least Squares) regression before. How does it relate to PCA? Both fit lines to data, but they minimize different things!\n",
    "\n",
    "**Key difference:**\n",
    "- **OLS:** Minimizes **vertical** distances (residuals in Y) -- assumes X is the predictor and Y is the outcome\n",
    "- **PCA:** Minimizes **perpendicular** distances to the line -- treats both variables symmetrically\n",
    "\n",
    "**Why the difference matters:**\n",
    "- OLS: \"Predict Y from X\" (asymmetric relationship)\n",
    "- PCA: \"Find the main direction of variation\" (symmetric—no predictor/outcome distinction)\n",
    "\n",
    "Let's visualize this difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddd597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit OLS: predict Inflation from Unemployment\n",
    "from sklearn.linear_model import LinearRegression\n",
    "ols = LinearRegression().fit(wb_eu[[\"Unemployment_rate\"]], wb_eu[\"Inflation_rate\"])\n",
    "print(f'OLS: Inflation = {ols.intercept_:.2f} + {ols.coef_[0]:.2f} × Unemployment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c341e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PCA vs OLS visually\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Original data points\n",
    "plt.scatter(wb_eu[\"Unemployment_rate\"], wb_eu[\"Inflation_rate\"], \n",
    "           alpha=0.6, s=80, label=\"Data\", color='steelblue', zorder=3)\n",
    "\n",
    "# PCA line (PC1 through the center of data)\n",
    "center = wb_eu.values.mean(axis=0)\n",
    "x_line = np.array([0, 18])\n",
    "y_pca = center[1] + (pca.components_[0][1] / pca.components_[0][0]) * (x_line - center[0])\n",
    "plt.plot(x_line, y_pca, color=\"darkred\", linewidth=2.5, label=\"PCA (minimizes perpendicular distances)\", zorder=2)\n",
    "\n",
    "# OLS line\n",
    "y_ols = ols.intercept_ + ols.coef_[0] * x_line\n",
    "plt.plot(x_line, y_ols, '--', color=\"darkgreen\", linewidth=2.5, \n",
    "        label=\"OLS (minimizes vertical distances)\", zorder=2)\n",
    "\n",
    "plt.xlabel(\"Unemployment rate (%)\")\n",
    "plt.ylabel(\"Inflation rate (%)\")\n",
    "plt.title(\"PCA vs OLS: Different Objectives\")\n",
    "plt.legend(loc='upper left', fontsize=9)\n",
    "plt.xlim(0, 18)\n",
    "plt.ylim(0, 18)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4527a41",
   "metadata": {},
   "source": [
    "**What we see:**\n",
    "- The lines are similar but not identical\n",
    "- PCA (red) treats both axes equally—it finds the \"main direction\" of the data cloud\n",
    "- OLS (green) assumes unemployment predicts inflation—it minimizes prediction errors in the Y direction\n",
    "\n",
    "**When to use which:**\n",
    "- Use **OLS** when you have a clear predictor → outcome relationship\n",
    "- Use **PCA** when you want to understand the overall structure or reduce dimensions without assuming causality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ab0fb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Summary: PCA Fundamentals\n",
    "\n",
    "**What we've learned so far:**\n",
    "\n",
    "1. **Motivation:** PCA reduces dimensions by projecting data onto directions that maximize variance\n",
    "2. **Mechanics:** Find PC1 (max variance), then PC2 (max remaining variance, perpendicular to PC1)\n",
    "3. **Loadings:** Show how original features combine to form PCs\n",
    "4. **Orthogonality:** PCs are perpendicular by construction (from sequential variance maximization)\n",
    "5. **Reconstruction error:** Measures information lost when using fewer PCs\n",
    "6. **Connection to OLS:** PCA minimizes perpendicular distances (symmetric), OLS minimizes vertical distances (asymmetric)\n",
    "\n",
    "**Next step:** Apply these concepts to a real dataset with many features. We'll discover a critical issue that requires **standardization** before PCA can work properly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f50f678",
   "metadata": {},
   "source": [
    "## Full PCA: Multiple economic indicators, countries from all over the world\n",
    "\n",
    "Now let's apply PCA to a richer dataset with **9 economic indicators** from countries worldwide. This will reveal both the power of PCA and a critical pitfall we must avoid.\n",
    "\n",
    "**The indicators:**\n",
    "- GDP per capita, GDP growth, Unemployment rate, Inflation rate\n",
    "- Trade (% of GDP), Foreign direct investment, Government debt\n",
    "- Life expectancy, Population growth\n",
    "\n",
    "Let's see what happens when we apply PCA directly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95b9775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the data\n",
    "df = pd.read_csv('../data/economic_indicators/countries_wb_2020_2023.csv')\n",
    "\n",
    "# Extract only the economic indicator features (drop categorical variables)\n",
    "wb_features = df.drop(columns=['Country', 'Income_Level','Region','Region_Broad',\n",
    "                               'is_BRICS','is_EU','is_G20','is_G7','is_OECD'])\n",
    "\n",
    "print(f'Dataset shape: {wb_features.shape}')\n",
    "print(f'Features: {list(wb_features.columns)}')\n",
    "wb_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68545da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca_full = PCA().fit(wb_features)\n",
    "\n",
    "print('Variance explained by each PC:')\n",
    "print(pca_full.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e72102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loadings to see what went wrong\n",
    "def visualize_pca_loadings(pca, data, title='PCA Components Loadings'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        pca.components_, \n",
    "        cmap='RdBu', center=0, annot=True, fmt='.2f', \n",
    "        xticklabels=data.columns, \n",
    "        yticklabels=[f\"PC{i+1}\" for i in range(pca.n_components_)],\n",
    "        cbar_kws={'label': 'Loading value'}\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "visualize_pca_loadings(pca_full, wb_features, 'PCA Loadings (WITHOUT scaling)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0208cd66",
   "metadata": {},
   "source": [
    "**Problem identified!** \n",
    "\n",
    "PC1 has a loading of ~1.00 on GDP_per_capita and ~0.00 on everything else. It's essentially just GDP_per_capita renamed! Why did this happen?\n",
    "\n",
    "Let's investigate the variance of each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27620763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare variances of each feature\n",
    "feature_variances = np.var(wb_features, axis=0).sort_values(ascending=False)\n",
    "print('Variance of each feature:\\n')\n",
    "print(feature_variances)\n",
    "print(f'\\nGDP_per_capita variance is {feature_variances.iloc[0] / feature_variances.iloc[1]:.0f}x larger than the next!')\n",
    "\n",
    "# Visualize the scale difference\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(feature_variances)), feature_variances.values, color='coral', alpha=0.8)\n",
    "plt.xticks(range(len(feature_variances)), feature_variances.index, rotation=45, ha='right')\n",
    "plt.ylabel('Variance')\n",
    "plt.title('Feature Variances (Original Scale)')\n",
    "plt.yscale('log')  # Log scale to see all bars\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40792022",
   "metadata": {},
   "source": [
    "**The root cause:** PCA finds directions of maximum variance. GDP_per_capita has vastly higher variance (because it's measured in dollars, ranging from ~500 to ~100,000) than percentage-based indicators (ranging from 0 to ~20).\n",
    "\n",
    "**Result:** PCA just picks GDP_per_capita because it has the most variance—not because it's the most important!\n",
    "\n",
    "**The solution: Standardization**\n",
    "\n",
    "Before applying PCA, we must **standardize** features to have:\n",
    "- Mean = 0 (centering)\n",
    "- Standard deviation = 1 (scaling)\n",
    "\n",
    "This puts all features on equal footing so PCA discovers true patterns, not just scale artifacts.\n",
    "\n",
    "Let's apply PCA correctly with standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c2e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.set_output(transform=\"pandas\")  # Keep output as DataFrame\n",
    "scaled_wb = scaler.fit_transform(wb_features)\n",
    "\n",
    "# Verify standardization worked\n",
    "print('After standardization:')\n",
    "print(f'Means (should be ~0): {np.mean(scaled_wb, axis=0).values}')\n",
    "print(f'Variances (should be 1.0): {np.var(scaled_wb, axis=0).values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA on standardized data\n",
    "scaled_pca = PCA().fit(scaled_wb)\n",
    "print('Variance explained by each PC (WITH scaling):')\n",
    "print(scaled_pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c2287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize explained variance (so called scree plot)\n",
    "def visualize_explained_variance(explained_variance, title='Explained Variance by Principal Component'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Bar chart for individual variance\n",
    "    plt.bar(range(1, len(explained_variance) + 1), explained_variance, \n",
    "           color='steelblue', alpha=0.8, label='Individual')\n",
    "    \n",
    "    # Line plot for cumulative variance\n",
    "    cumulative = np.cumsum(explained_variance)\n",
    "    plt.plot(range(1, len(explained_variance) + 1), cumulative, \n",
    "            color=\"darkred\", marker=\"o\", linewidth=2.5, markersize=8, label='Cumulative')\n",
    "    \n",
    "    # Reference lines for 80% and 90%\n",
    "    plt.axhline(0.8, linestyle='--', color='orange', alpha=0.7, linewidth=1.5, label='80% threshold')\n",
    "    plt.axhline(0.9, linestyle='--', color='red', alpha=0.7, linewidth=1.5, label='90% threshold')\n",
    "\n",
    "    plt.xlabel(\"Principal Component\")\n",
    "    plt.ylabel(\"Explained Variance Ratio\")\n",
    "    plt.title(title)\n",
    "    plt.xticks(range(1, len(explained_variance) + 1))\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "visualize_explained_variance(scaled_pca.explained_variance_ratio_, 'Scree Plot: Variance Explained (Scaled Data)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeee30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loadings for scaled PCA\n",
    "visualize_pca_loadings(scaled_pca, scaled_wb, 'PCA Loadings (WITH scaling) - Much Better!')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0144f9a",
   "metadata": {},
   "source": [
    "**Much better!** Now we see meaningful patterns:\n",
    "\n",
    "**PC1 (~65-75% of variance):** \"Development level\"\n",
    "- High positive loadings: Life_expectancy (0.45), GDP_per_capita (0.43), Internet_users (0.42)\n",
    "- Interpretation: Distinguishes developed from developing countries\n",
    "\n",
    "**PC2 (~10-15% of variance):** \"Domestic welfare vs trade openness\"\n",
    "- High positive loadings: Health_spending (0.54), Unemployment (0.49)\n",
    "- High negative loadings: Trade_pct_GDP (-0.54)\n",
    "- Interpretation: Contrasts countries with high domestic spending/unemployment against trade-oriented economies\n",
    "\n",
    "**Key insight:** Scaling revealed the true economic structure hidden in the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4bb8e",
   "metadata": {},
   "source": [
    "### Visualizing countries in PC space\n",
    "\n",
    "Now let's see where countries are positioned in the new PC coordinate system. Countries with similar economic profiles should cluster together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c02d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA: fit and transform in one step (same as we did for the 2-features case)\n",
    "scaled_wb_pca = scaled_pca.fit_transform(scaled_wb)\n",
    "\n",
    "# Create scores DataFrame (countries in PC space)\n",
    "scores = pd.DataFrame(\n",
    "    scaled_wb_pca, \n",
    "    index=df['Country'], \n",
    "    columns=[f'PC{i+1}' for i in range(scaled_wb_pca.shape[1])]\n",
    ")\n",
    "\n",
    "# Create loadings DataFrame (feature contributions to PCs)\n",
    "loadings = pd.DataFrame(\n",
    "    scaled_pca.components_.T, \n",
    "    index=scaled_wb.columns, \n",
    "    columns=[f'PC{i+1}' for i in range(scaled_pca.components_.shape[0])]\n",
    ")\n",
    "\n",
    "print('Scores shape:', scores.shape)\n",
    "print('Loadings shape:', loadings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d806bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of countries in PC1-PC2 space\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(scores['PC1'], scores['PC2'], alpha=0.5, s=80, color='steelblue', edgecolors='white', linewidth=0.5)\n",
    "\n",
    "# Label a diverse selection of countries to show clustering patterns\n",
    "to_label = [c for c in [\n",
    "    # Developed economies\n",
    "    'USA', 'DEU', 'NOR', 'SWE', 'CHE', 'JPN', 'AUS', 'CAN', 'GBR', 'FRA',\n",
    "    # Emerging markets\n",
    "    'CHN', 'IND', 'BRA', 'RUS', 'MEX', 'TUR', 'IDN', 'ZAF',\n",
    "    # Developing countries\n",
    "    'PAK', 'NGA', 'BGD', 'ETH', 'KEN', 'VNM',\n",
    "    # Special cases\n",
    "    'ARG', 'SGP', 'HUN', 'IRL', 'LUX'\n",
    "] if c in scores.index]\n",
    "\n",
    "for c in to_label:\n",
    "    plt.annotate(c, (scores.loc[c, 'PC1'], scores.loc[c, 'PC2']), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9, fontweight='bold', alpha=0.8)\n",
    "\n",
    "plt.xlabel(f'PC1: Development Level ({scaled_pca.explained_variance_ratio_[0]*100:.1f}% var)')\n",
    "plt.ylabel(f'PC2: Domestic Welfare vs Trade Openness ({scaled_pca.explained_variance_ratio_[1]*100:.1f}% var)')\n",
    "plt.title('Countries in Principal Component Space\\n(Similar countries cluster together)', fontsize=14, fontweight='bold')\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "plt.axvline(0, color='gray', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8c3e2e",
   "metadata": {},
   "source": [
    "**Observations from the country scatter:**\n",
    "- **Horizontal axis (PC1 - Development level):** \n",
    "  - Right side: Developed economies (USA, DEU, NOR, SWE, CHE, JPN, AUS, GBR, FRA, CAN)\n",
    "  - Left side: Developing countries (PAK, NGA, BGD, ETH, KEN)\n",
    "  - Middle: Emerging markets (CHN, IND, BRA, MEX, TUR, IDN, ZAF)\n",
    "- **Vertical axis (PC2 - Domestic welfare vs trade openness):**\n",
    "  - Positive: Countries with high domestic spending/unemployment (e.g., ZAF, some European countries)\n",
    "  - Negative: Trade-oriented economies (SGP, IRL, LUX)\n",
    "- **Clusters:** Countries with similar economic profiles naturally group together:\n",
    "  - Nordic countries (NOR, SWE) cluster near other developed economies\n",
    "  - BRICS countries (CHN, IND, BRA, RUS, ZAF) show varied positions reflecting their diverse profiles\n",
    "  - Sub-Saharan African countries (NGA, ETH, KEN) cluster on the left (lower development)\n",
    "  - Small open economies (SGP, IRL, LUX) show negative PC2 (high trade orientation)\n",
    "\n",
    "**Achievement:** We've compressed 9 dimensions into 2, making invisible patterns visible!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d675ec5",
   "metadata": {},
   "source": [
    "### Biplot: Combining Countries and Indicators\n",
    "\n",
    "A **biplot** overlays both countries (points) and indicators (arrows) in the same PC space. This powerful visualization shows:\n",
    "- Where countries are positioned\n",
    "- Which indicators drive those positions\n",
    "- How indicators relate to each other\n",
    "\n",
    "**How to read it:**\n",
    "- **Arrow direction:** Shows which way that indicator \"pulls\" countries\n",
    "- **Arrow length:** Longer = more influence on the visible PCs\n",
    "- **Arrow angles:** Small angle = indicators are correlated; opposite directions = negatively correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9953bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_biplot(scores_df, loadings_df, explained_variance_ratio, pc1=0, pc2=1, scale_factor=4):\n",
    "    \"\"\"Create a biplot showing both countries (scores) and indicators (loadings)\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    pc1_name = f'PC{pc1+1}'\n",
    "    pc2_name = f'PC{pc2+1}'\n",
    "\n",
    "    # Plot country scores\n",
    "    ax.scatter(scores_df[pc1_name], scores_df[pc2_name], alpha=0.55, s=70, \n",
    "              color='steelblue', edgecolors='white', linewidth=0.5, label='Countries')\n",
    "\n",
    "    # Label selected countries\n",
    "    to_label = [c for c in ['USA', 'CHN', 'IND', 'NOR', 'PAK', 'ARG', 'SGP', 'HUN', 'NGA', 'DEU'] \n",
    "               if c in scores_df.index]\n",
    "    for c in to_label:\n",
    "        ax.annotate(c, (scores_df.loc[c, pc1_name], scores_df.loc[c, pc2_name]), \n",
    "                   xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "    # Plot indicator vectors (arrows)\n",
    "    for indicator in loadings_df.index:\n",
    "        x = loadings_df.loc[indicator, pc1_name] * scale_factor\n",
    "        y = loadings_df.loc[indicator, pc2_name] * scale_factor\n",
    "        ax.arrow(0, 0, x, y, color='darkred', alpha=0.7, head_width=0.12, head_length=0.12, \n",
    "                linewidth=2.5, length_includes_head=True)\n",
    "        # Clean up indicator names for display\n",
    "        label = indicator.replace('_', ' ')\n",
    "        ax.text(x * 1.15, y * 1.15, label, color='darkred', fontsize=10, \n",
    "               ha='center', va='center', fontweight='bold',\n",
    "               bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7, edgecolor='darkred'))\n",
    "\n",
    "    ax.set_xlabel(f'{pc1_name} ({explained_variance_ratio[pc1]*100:.1f}% variance)', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(f'{pc2_name} ({explained_variance_ratio[pc2]*100:.1f}% variance)', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Biplot: Countries (blue) + Economic Indicators (red arrows)', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.axhline(0, color='gray', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "    ax.axvline(0, color='gray', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "create_biplot(scores, loadings, scaled_pca.explained_variance_ratio_, pc1=0, pc2=1, scale_factor=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39e36c",
   "metadata": {},
   "source": [
    "**Interpreting the biplot:**\n",
    "- **GDP_per_capita and Life_expectancy** point right → developed countries (USA, DEU, NOR) are on that side\n",
    "- **Population_growth** points left → developing countries (IND, PAK, NGA) have higher population growth\n",
    "- **Trade_pct_GDP** points up → captures Singapore's trade-oriented economy (SGP high on PC2)\n",
    "- **Correlated indicators** point in similar directions (e.g., GDP and Life expectancy)\n",
    "\n",
    "This single plot tells the economic story of how countries differ!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cb7fa9",
   "metadata": {},
   "source": [
    "### Robustness Check: Do outliers change PCA?\n",
    "\n",
    "Some countries (Argentina, Singapore, Hungary) have unusual economic profiles. Do these outliers distort our PCA results? Let's check by refitting PCA without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f90a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers and refit PCA\n",
    "outliers = [c for c in ['ARG', 'SGP', 'HUN'] if c in scores.index]\n",
    "print(f'Removing outliers: {outliers}')\n",
    "\n",
    "mask = ~df['Country'].isin(outliers)\n",
    "wb_features_wo = wb_features.loc[mask]\n",
    "scaled_wb_wo = StandardScaler().fit_transform(wb_features_wo)\n",
    "pca_wo = PCA().fit(scaled_wb_wo)\n",
    "\n",
    "# Compare results\n",
    "print(f'\\n--- Comparison: All countries vs. Without outliers ---')\n",
    "print(f'PC1 variance explained (all):        {scaled_pca.explained_variance_ratio_[0]:.1%}')\n",
    "print(f'PC1 variance explained (no outliers): {pca_wo.explained_variance_ratio_[0]:.1%}')\n",
    "\n",
    "# Compare PC1 loadings\n",
    "common = wb_features_wo.columns\n",
    "loadings_wo = pd.Series(pca_wo.components_[0], index=common)\n",
    "corr = np.corrcoef(loadings.loc[common, 'PC1'].values, loadings_wo.values)[0, 1]\n",
    "print(f'\\nCorrelation between PC1 loadings: {corr:.3f}')\n",
    "print('(Values close to ±1 mean loadings barely changed)')\n",
    "\n",
    "print('\\n✓ Result: Outliers have minimal impact on PCA structure')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd02f3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: PCA\n",
    "\n",
    "### What we learned\n",
    "\n",
    "**1. Motivation: Dimensionality Reduction**\n",
    "- Goal: Summarize many features with fewer dimensions while preserving information\n",
    "- Method: Project data onto directions that maximize variance\n",
    "- Why variance? It measures how spread out (informative) the projected data is\n",
    "\n",
    "**2. PCA Mechanics**\n",
    "- PC1: Direction of maximum variance\n",
    "- PC2: Maximum remaining variance, perpendicular to PC1\n",
    "- Continues sequentially for all dimensions\n",
    "\n",
    "**3. Key Characteristics**\n",
    "\n",
    "| Property | Explanation | Why It Matters |\n",
    "|----------|-------------|----------------|\n",
    "| **Loadings** | Weights showing how features combine into PCs | Interprets what each PC represents |\n",
    "| **Orthogonality** | PCs are perpendicular (uncorrelated) | Each PC captures independent information |\n",
    "| **Explained Variance** | Proportion of total variance each PC captures | Guides how many PCs to keep |\n",
    "| **Reconstruction Error** | Information lost when using fewer PCs | Quantifies dimension reduction trade-off |\n",
    "\n",
    "**4. Connection to OLS**\n",
    "- OLS: Minimizes vertical distances (predicts Y from X)\n",
    "- PCA: Minimizes perpendicular distances (symmetric treatment)\n",
    "- Use OLS for prediction, PCA for understanding structure\n",
    "\n",
    "**5. Critical Requirement: Standardization**\n",
    "- **Problem:** Features with larger scales dominate PCA\n",
    "- **Solution:** Standardize (mean=0, std=1) before PCA\n",
    "- **Always remember:** Scale first, then PCA!\n",
    "\n",
    "**6. Practical Application**\n",
    "- Compressed 9 economic indicators → 2 PCs\n",
    "- PC1 captured \"development level\" (~65-75% variance)\n",
    "- PC2 captured \"economic openness\" (~10-15% variance)\n",
    "- Biplot revealed economic structure across countries\n",
    "\n",
    "### When to use PCA\n",
    "\n",
    "**✅ PCA works well when:**\n",
    "- Features are correlated (redundant information to compress)\n",
    "- Need visualization (reduce to 2-3D)\n",
    "- Want to discover underlying patterns\n",
    "- Building models and want uncorrelated features\n",
    "\n",
    "**❌ Be cautious when:**\n",
    "- Features have vastly different scales (standardize first!)\n",
    "- Features are already uncorrelated (little to gain)\n",
    "- Individual feature interpretation is critical (PCs are combinations)\n",
    "- Need non-linear patterns (PCA is linear)\n",
    "\n",
    "**The big picture:** PCA is one powerful tool for fighting the curse of dimensionality. By finding the directions of maximum variance, it lets us work in lower dimensions while preserving the information that matters most."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
