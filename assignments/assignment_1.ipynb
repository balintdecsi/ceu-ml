{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "## General information\n",
    "\n",
    "Please give short (2-3 sentences) interpretations / explanations to your answers, not only the program code and outputs. Be concise and focused (less could be more ;)).\n",
    "\n",
    "Grades will be distributed with the following rule: from the points you earn, you get 100% if you submit until the due date (**2026-02-25 12:00 CET**), 50% within 24 hours past due date, and 0% after that.\n",
    "\n",
    "## Task 1: Gene expression data (5 points)\n",
    "\n",
    "From the ISLR website, we can download a gene expression data set (`Ch10Ex11.csv`) that consists of 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group.\n",
    "\n",
    "We would have no chance to estimate any model on these 1,000 features. However, we could reduce the dimensionality with PCA. Then, we could look at the relation of the first few principal components (that captures part of the variance of all 1,000 features) and the outcome.\n",
    "\n",
    "Note that as we only have 40 observations we can only define a subspace with at most 39 dimensions. Think of it like trying to define a volume in 3D space using only 2 points - you can only define a line, not a full 3D space. Technically, the sklearn PCA implementation will result in 40 components, but the last one will have a variance of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "url = 'https://www.statlearning.com/s/Ch10Ex11.csv'\n",
    "genes = pd.read_csv(url, header=None)\n",
    "\n",
    "# Transpose the dataframe and convert to pandas DataFrame\n",
    "genes = genes.T\n",
    "genes = pd.DataFrame(genes)\n",
    "print('Dimensions of genes dataframe:', genes.shape)\n",
    "\n",
    "# Define health_status\n",
    "health_status = ['healthy'] * 20 + ['diseased'] * 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always\"></div>\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. Compute the variance of each feature and plot a histogram of those variances. Based on the histogram, are the features similarly scaled? Would you standardize the features before applying PCA? Explain your reasoning. (*Hint*: treat features as similarly scaled if their variances differ by at most an order of magnitude.)\n",
    "\n",
    "2. Apply PCA to the full dataset. How many principal components are required to explain at least 90\\% of the total variance? How much variance is captured by the first two principal components?\n",
    "\n",
    "3. Create a scatter plot of the first two component scores (the data projected onto the first two principal components). Color the points by patient health status. Do these components help distinguish healthy from diseased patients? Which component separates the groups more clearly?\n",
    "\n",
    "4. For the principal component that best separates healthy and diseased patients, identify the genes with the largest absolute loadings (the strongest contributors). Briefly describe how you selected them. (*Hint*: examine the component loadings.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 2: PCA on uncorrelated features (5 points)\n",
    "\n",
    "In this task you will simulate data where the data-generating process has no correlations between features (in math terms: the population covariance is the identity matrix).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation\n",
    "import numpy as np\n",
    "\n",
    "# rng = <TODO>\n",
    "rng = np.random.default_rng(seed=0)  # for reproducibility\n",
    "n = 100\n",
    "p = 50\n",
    "X = rng.normal(loc=0.0, scale=1.0, size=(n, p))  # population covariance = I_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always\"></div>\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. Run PCA on the generated data (100 samples, 50 features). How many components are needed to explain at least 90% of the variance? Does this match your theoretical expectation given the data-generating process?\n",
    "\n",
    "2. Split the data into an 80â€“20% train/test split. Fit PCA using the *train* set only. How many components are required to explain at least 90% of the variance on the training set? Compute the reconstruction error on the training set like we did in class and explain the result. Then, compute the reconstruction error on the test set and compare it to the training error.\n",
    "\n",
    "3. Calculate the reconstruction error on the test set. Compare it to the train set.\n",
    "\n",
    "4. For a range of component counts, compute reconstruction error on both train and test sets. Are the first few principal components equally effective on the test set as on the train set? Explain any differences.\n",
    "\n",
    "5. Based on your results, how many components would you choose to summarize this dataset? Justify your choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
