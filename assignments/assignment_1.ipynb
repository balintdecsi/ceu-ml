{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "## General information\n",
        "\n",
        "Please give short (2-3 sentences) interpretations / explanations to your answers, not only the program code and outputs. Be concise and focused, especially in the AI-world. We take this seriously and will downgrade excessive verbosity.\n",
        "\n",
        "Grades will be distributed with the following rule: from the points you earn, you get 100% if you submit until the due date (**2026-02-25 12:00 CET**), 50% within 24 hours past due date, and 0% after that.\n",
        "\n",
        "## Task 1: Gene expression data (3 points)\n",
        "\n",
        "From the ISLR website, we can download a **gene expression data** set (`Ch10Ex11.csv`) that consists of 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group.\n",
        "\n",
        "We would have no chance to estimate any model on these 1,000 features. However, we could reduce the dimensionality with PCA. Then, we could look at the relation of the first few principal components (that captures part of the variance of all 1,000 features) and the outcome.\n",
        "\n",
        "Note that as we only have 40 observations we can only define a subspace with at most 39 dimensions. Think of it like trying to define a volume in 3D space using only 2 points - you can only define a line, not a full 3D space. Technically, the sklearn PCA implementation will result in 40 components, but the last one will have a variance of 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the data\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "url = 'https://www.statlearning.com/s/Ch10Ex11.csv'\n",
        "genes = pd.read_csv(url, header=None)\n",
        "\n",
        "# Transpose the dataframe and convert to pandas DataFrame\n",
        "genes = genes.T\n",
        "genes = pd.DataFrame(genes)\n",
        "print('Dimensions of genes dataframe:', genes.shape)\n",
        "\n",
        "# Define health_status\n",
        "health_status = ['healthy'] * 20 + ['diseased'] * 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"page-break-after: always\"></div>\n",
        "\n",
        "### Questions\n",
        "\n",
        "1. Compute the variance of each feature and plot a histogram of those variances. Based on the histogram, are the features similarly scaled? Would you standardize the features before applying PCA? Explain your reasoning. (*Hint*: treat features as similarly scaled if their variances differ by at most an order of magnitude.)\n",
        "\n",
        "2. Apply PCA to the full dataset. How many principal components are required to explain at least 90\\% of the total variance? How much variance is captured by the first two principal components?\n",
        "\n",
        "3. Create a scatter plot of the first two component scores (the data projected onto the first two principal components). Color the points by patient health status. Do these components help distinguish healthy from diseased patients? Which component separates the groups more clearly?\n",
        "\n",
        "4. For the principal component that best separates healthy and diseased patients, identify the genes with the largest absolute loadings (the strongest contributors). Briefly describe how you selected them. (*Hint*: examine the component loadings.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Compute variance of each feature (gene)\n",
        "variances = genes.var(axis=0)\n",
        "\n",
        "# Plot histogram of variances\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(variances, bins=30, color='skyblue', edgecolor='black')\n",
        "plt.xlabel('Feature Variance')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Feature Variances')\n",
        "plt.show()\n",
        "\n",
        "# Interpretation\n",
        "min_var = variances.min()\n",
        "max_var = variances.max()\n",
        "order_magnitude_ratio = max_var / min_var if min_var > 0 else np.inf\n",
        "\n",
        "print(f\"Minimum variance: {min_var:.4f}\")\n",
        "print(f\"Maximum variance: {max_var:.4f}\")\n",
        "print(f\"Ratio (max/min): {order_magnitude_ratio:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** The features are similarly scaled, since their variances differ by less than an order of magnitude. Standardization before PCA might not be strictly necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca_genes = PCA()\n",
        "genes_pca = pca_genes.fit_transform(genes)\n",
        "\n",
        "cumulative_var = np.cumsum(pca_genes.explained_variance_ratio_)\n",
        "n_components_90 = np.argmax(cumulative_var >= 0.90) + 1\n",
        "\n",
        "print(f\"Components needed for >=90% variance: {n_components_90}\")\n",
        "print(f\"\\nVariance captured by first two PCs: {cumulative_var[1]*100:.2f}%\")\n",
        "print(f\"  PC1: {pca_genes.explained_variance_ratio_[0]*100:.2f}%\")\n",
        "print(f\"  PC2: {pca_genes.explained_variance_ratio_[1]*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Many principal components are needed to reach 90% of the total variance. The first two PCs nonetheless capture a meaningful share, indicating some low-dimensional structure in the gene expression data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.patches as mpatches\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "colors = ['steelblue' if s == 'healthy' else 'coral' for s in health_status]\n",
        "plt.scatter(genes_pca[:, 0], genes_pca[:, 1], c=colors, s=60, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "healthy_patch = mpatches.Patch(color='steelblue', label='Healthy')\n",
        "diseased_patch = mpatches.Patch(color='coral', label='Diseased')\n",
        "plt.legend(handles=[healthy_patch, diseased_patch])\n",
        "\n",
        "plt.xlabel(f'PC1 ({pca_genes.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
        "plt.ylabel(f'PC2 ({pca_genes.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
        "plt.title('Gene Expression: First Two Principal Components')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** PC1 clearly separates the healthy and diseased groups into two distinct clusters, making it the more informative component for distinguishing health status. PC2 captures within-group variation but does not meaningfully separate the two groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pc1_loadings = pca_genes.components_[0]\n",
        "\n",
        "n_top = 10\n",
        "top_gene_indices = np.argsort(np.abs(pc1_loadings))[::-1][:n_top]\n",
        "\n",
        "print(f\"Top {n_top} genes by absolute loading on PC1:\")\n",
        "for rank, idx in enumerate(top_gene_indices, 1):\n",
        "    print(f\"  {rank}. Gene {idx}: loading = {pc1_loadings[idx]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Since PC1 separates the groups most clearly, I examined its loadings. I selected the top contributing genes by sorting the absolute values of the PC1 loadings in descending order. These genes have the largest influence on the component that distinguishes healthy from diseased patients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: PCA on simulated data (3 points)\n",
        "\n",
        "In this exercise, you will work on simulated features. This simulated data have **mixed correlation structure**: some features cluster together (correlated), while others are independent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data generation with mixed correlation structure\n",
        "import numpy as np\n",
        "\n",
        "rng = np.random.default_rng(seed=20260218)  # for reproducibility\n",
        "n = 100\n",
        "p = 50\n",
        "\n",
        "# Create 5 groups of 10 correlated features each\n",
        "# Within each group, features are correlated with each other\n",
        "# Between groups, features are independent\n",
        "\n",
        "X = np.zeros((n, p))\n",
        "\n",
        "# Generate 5 independent latent factors (one per group)\n",
        "n_groups = 5\n",
        "features_per_group = 10\n",
        "latent_factors = rng.normal(loc=0.0, scale=1.0, size=(n, n_groups))\n",
        "\n",
        "for group in range(n_groups):\n",
        "    start_idx = group * features_per_group\n",
        "    end_idx = start_idx + features_per_group\n",
        "    \n",
        "    # Each feature in the group is the latent factor plus independent noise\n",
        "    # This creates correlation within the group\n",
        "    for i in range(features_per_group):\n",
        "        noise = rng.normal(loc=0.0, scale=0.5, size=n)  # smaller noise = higher correlation\n",
        "        X[:, start_idx + i] = latent_factors[:, group] + noise\n",
        "\n",
        "print(f'Generated data shape: {X.shape}')\n",
        "print(f'Data structure: {n_groups} groups of {features_per_group} correlated features each')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"page-break-after: always\"></div>\n",
        "\n",
        "### Questions\n",
        "\n",
        "1. Compute the correlation matrix of the generated features and visualize it as a heatmap. Describe the correlation structure you observe. Does it match the data-generating process?\n",
        "\n",
        "2. Run PCA on the full dataset (100 samples, 50 features). How many components are needed to explain at least 90% of the variance? \n",
        "\n",
        "3. Plot the explained variance ratio for the first 20 components. What does the shape of this curve tell you about the data structure? (*Hint*: look for an \"elbow\" or plateaus in the curve.)\n",
        "\n",
        "4. Split the data into 80–20% train/test sets. For a range of component counts, compute reconstruction errors on both sets like we did in class. Plot these errors. Based on the plot, how many components would you choose to best summarize this dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "corr_matrix = np.corrcoef(X, rowvar=False)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, cmap='RdBu', center=0, vmin=-1, vmax=1)\n",
        "plt.title('Correlation Matrix of Simulated Features')\n",
        "plt.xlabel('Feature Index')\n",
        "plt.ylabel('Feature Index')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** The heatmap shows a clear block-diagonal structure with 5 blocks of 10 highly correlated features each, and near-zero correlations between blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca_sim = PCA()\n",
        "X_sim_pca = pca_sim.fit_transform(X)\n",
        "\n",
        "cumulative_var_sim = np.cumsum(pca_sim.explained_variance_ratio_)\n",
        "n_comp_90_sim = np.argmax(cumulative_var_sim >= 0.90) + 1\n",
        "\n",
        "print(f\"Components needed for >=90% variance: {n_comp_90_sim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Only a small number of components is needed to reach 90% of the variance, because the 5 underlying latent factors create strong within-group correlations that PCA can efficiently compress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(range(1, 21), pca_sim.explained_variance_ratio_[:20],\n",
        "        color='steelblue', alpha=0.8, label='Individual')\n",
        "\n",
        "cumulative_20 = np.cumsum(pca_sim.explained_variance_ratio_[:20])\n",
        "plt.plot(range(1, 21), cumulative_20,\n",
        "         color='darkred', marker='o', linewidth=2.5, markersize=8, label='Cumulative')\n",
        "\n",
        "plt.axhline(0.9, linestyle='--', color='orange', alpha=0.7, linewidth=1.5, label='90% threshold')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Scree Plot: Explained Variance (Simulated Data)')\n",
        "plt.legend()\n",
        "plt.xticks(range(1, 21))\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** The scree plot shows a clear elbow after approximately 5 components, corresponding to the 5 underlying latent factors. The first 5 PCs capture the bulk of the variance, while subsequent components contribute only marginally, indicating that the data's true dimensionality is around 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
        "\n",
        "def calculate_reconstruction_error(X_orig, X_reconstructed):\n",
        "    \"\"\"Mean squared distance between original and reconstructed data.\"\"\"\n",
        "    return np.mean(np.sum(np.square(X_orig - X_reconstructed), axis=1))\n",
        "\n",
        "component_range = range(1, X_train.shape[1] + 1)\n",
        "train_errors = []\n",
        "test_errors = []\n",
        "\n",
        "for n_comp in component_range:\n",
        "    pca_k = PCA(n_components=n_comp)\n",
        "    X_train_pca = pca_k.fit_transform(X_train)\n",
        "    X_train_recon = pca_k.inverse_transform(X_train_pca)\n",
        "\n",
        "    X_test_pca = pca_k.transform(X_test)\n",
        "    X_test_recon = pca_k.inverse_transform(X_test_pca)\n",
        "\n",
        "    train_errors.append(calculate_reconstruction_error(X_train, X_train_recon))\n",
        "    test_errors.append(calculate_reconstruction_error(X_test, X_test_recon))\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(list(component_range), train_errors, marker='o', label='Train', markersize=4)\n",
        "plt.plot(list(component_range), test_errors, marker='s', label='Test', markersize=4)\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Reconstruction Error')\n",
        "plt.title('Reconstruction Error vs Number of Components (Simulated Data)')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** The reconstruction error drops sharply for the first ~5 components and then plateaus, confirming that approximately 5 components best summarize this dataset. Train and test errors follow very similar trends, indicating that the PCA structure generalizes well to unseen data when there is genuine correlation structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Task 3: PCA on uncorrelated features (4 points)\n",
        "\n",
        "In this task you will simulate data where the data-generating process has **no correlations** between features (in math terms: the population covariance is the identity matrix).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data generation\n",
        "import numpy as np\n",
        "\n",
        "rng = np.random.default_rng(seed=20260218)  # for reproducibility\n",
        "n = 100\n",
        "p = 50\n",
        "X = rng.normal(loc=0.0, scale=1.0, size=(n, p))  # population covariance = I_p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"page-break-after: always\"></div>\n",
        "\n",
        "### Questions\n",
        "\n",
        "1. Run PCA on the generated data (100 samples, 50 features). How many components are needed to explain at least 90% of the variance? Does this match your theoretical expectation given the data-generating process? (*Hint*: looking at the correlation heatmap like you did in the previous task could help.)\n",
        "\n",
        "2. Split the data into an 80–20% train/test split. Fit PCA using the *train* set only. How many components are required to explain at least 90% of the variance on the training set? Compute the reconstruction error on the training set like we did in class and explain the result. Then, compute the reconstruction error on the test set and compare it to the training error.\n",
        "\n",
        "3. For a range of component counts, compute reconstruction error on both train and test sets. Are the first few principal components equally effective on the test set as on the train set? Explain any differences.\n",
        "\n",
        "4. Compare your findings across all three tasks: \n",
        "   - Task 1 (gene expression): high-dimensional real data\n",
        "   - Task 2 (partial correlation): mixed structure\n",
        "   - Task 3 (uncorrelated): no correlation structure  \n",
        "   \n",
        "   In which scenario is PCA most effective for dimensionality reduction? Explain why correlation structure matters for PCA's effectiveness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_matrix_uncorr = np.corrcoef(X, rowvar=False)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix_uncorr, cmap='RdBu', center=0, vmin=-1, vmax=1)\n",
        "plt.title('Correlation Matrix (Uncorrelated Features)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "pca_uncorr = PCA()\n",
        "X_uncorr_pca = pca_uncorr.fit_transform(X)\n",
        "\n",
        "cumulative_var_uncorr = np.cumsum(pca_uncorr.explained_variance_ratio_)\n",
        "n_comp_90_uncorr = np.argmax(cumulative_var_uncorr >= 0.90) + 1\n",
        "\n",
        "print(f\"Components needed for >=90% variance: {n_comp_90_uncorr} out of {len(pca_uncorr.explained_variance_ratio_)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** The correlation heatmap shows no meaningful block structure, only weak spurious sample correlations. Consequently, nearly all components are needed to reach 90% of the variance. This matches expectations: with truly uncorrelated features, each component captures roughly the same amount of variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
        "\n",
        "pca_train = PCA()\n",
        "pca_train.fit(X_train)\n",
        "\n",
        "cumulative_var_train = np.cumsum(pca_train.explained_variance_ratio_)\n",
        "n_comp_90_train = np.argmax(cumulative_var_train >= 0.90) + 1\n",
        "print(f\"Components for >=90% variance on training set: {n_comp_90_train}\")\n",
        "\n",
        "pca_90 = PCA(n_components=n_comp_90_train)\n",
        "X_train_pca = pca_90.fit_transform(X_train)\n",
        "X_train_recon = pca_90.inverse_transform(X_train_pca)\n",
        "\n",
        "X_test_pca = pca_90.transform(X_test)\n",
        "X_test_recon = pca_90.inverse_transform(X_test_pca)\n",
        "\n",
        "total_var_train = np.sum(np.var(X_train, axis=0))\n",
        "total_var_test = np.sum(np.var(X_test, axis=0))\n",
        "\n",
        "train_error = calculate_reconstruction_error(X_train, X_train_recon)\n",
        "test_error = calculate_reconstruction_error(X_test, X_test_recon)\n",
        "\n",
        "print(f\"\\nReconstruction error (train): {train_error:.4f}\")\n",
        "print(f\"Relative error (train): {train_error / total_var_train * 100:.1f}%\")\n",
        "print(f\"\\nReconstruction error (test): {test_error:.4f}\")\n",
        "print(f\"Relative error (test): {test_error / total_var_test * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Even on the training set, many components are needed for 90% variance since there is no true correlation structure to exploit. The test reconstruction error is noticeably higher than the training error, because PCA captures spurious correlations in the training data that do not generalize to unseen observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "component_range = range(1, X_train.shape[1] + 1)\n",
        "train_errors = []\n",
        "test_errors = []\n",
        "\n",
        "for n_comp in component_range:\n",
        "    pca_k = PCA(n_components=n_comp)\n",
        "    X_train_pca = pca_k.fit_transform(X_train)\n",
        "    X_train_recon = pca_k.inverse_transform(X_train_pca)\n",
        "\n",
        "    X_test_pca = pca_k.transform(X_test)\n",
        "    X_test_recon = pca_k.inverse_transform(X_test_pca)\n",
        "\n",
        "    train_errors.append(calculate_reconstruction_error(X_train, X_train_recon))\n",
        "    test_errors.append(calculate_reconstruction_error(X_test, X_test_recon))\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(list(component_range), train_errors, marker='o', label='Train', markersize=4)\n",
        "plt.plot(list(component_range), test_errors, marker='s', label='Test', markersize=4)\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Reconstruction Error')\n",
        "plt.title('Reconstruction Error vs Number of Components (Uncorrelated Data)')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** The first few principal components are less effective on the test set than on the training set, as shown by the consistently higher test reconstruction error. This gap arises because, with no true correlation structure, PCA overfits to noise patterns in the training data, and these noise-driven components do not transfer well to unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "PCA is most effective for dimensionality reduction when features are correlated (Task 2), since it can compress groups of correlated features into single components, requiring only ~5 PCs for 90% variance. With real gene expression data (Task 1), PCA finds meaningful structure (PC1 separates health groups) but still needs many components for 90% coverage. With uncorrelated features (Task 3), PCA offers essentially no benefit: nearly all components are needed, and the components overfit to noise, generalizing poorly to test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
<<<<<<< HEAD
=======
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "## General information\n",
    "\n",
    "Please give short (2-3 sentences) interpretations / explanations to your answers, not only the program code and outputs. Be concise and focused, especially in the AI-world. We take this seriously and will downgrade excessive verbosity. You should submit only the converted `.pdf` file to Moodle.\n",
    "\n",
    "Grades will be distributed with the following rule: from the points you earn, you get 100% if you submit until the due date (**2026-02-25 12:00 CET**), 50% within 24 hours past due date, and 0% after that. \n",
    "\n",
    "## Task 1: Gene expression data (3 points)\n",
    "\n",
    "From the ISLR website, we can download a **gene expression data** set (`Ch10Ex11.csv`) that consists of 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group.\n",
    "\n",
    "We would have no chance to estimate any model on these 1,000 features. However, we could reduce the dimensionality with PCA. Then, we could look at the relation of the first few principal components (that captures part of the variance of all 1,000 features) and the outcome.\n",
    "\n",
    "Note that as we only have 40 observations we can only define a subspace with at most 39 dimensions. Think of it like trying to define a volume in 3D space using only 2 points - you can only define a line, not a full 3D space. Technically, the sklearn PCA implementation will result in 40 components, but the last one will have a variance of 0."
   ]
>>>>>>> courseware
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
