{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08cc7538",
   "metadata": {},
   "source": [
    "# Class 4: The Bias-Variance Trade-off\n",
    "\n",
    "**Big idea:** More complex models aren't always better. The best model balances *accuracy* (low bias) with *stability* (low variance).\n",
    "\n",
    "Today we will:\n",
    "- Understand bias and variance through **simulation** (not just formulas)\n",
    "- See why simple models can outperform complex ones\n",
    "- Recap **regularization** (Lasso) as a tool to reduce variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173607e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "prng = np.random.RandomState(20260225)\n",
    "\n",
    "%precision 3\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136cc179",
   "metadata": {},
   "source": [
    "## Why do simple models sometimes win?\n",
    "\n",
    "In the previous class on clustering, we saw that adding more features doesn't always help (curse of dimensionality). Here we'll discover a related phenomenon: **adding more complexity to our model doesn't always help either**.\n",
    "\n",
    "Consider this puzzle: If we know the true relationship is quadratic ($Y = X^2 - 1.5X + \\varepsilon$), shouldn't a quadratic model always outperform a linear one?\n",
    "\n",
    "**Spoiler:** Not necessarily! Let's understand why through the **bias-variance decomposition**.\n",
    "\n",
    "### The Setup\n",
    "\n",
    "Suppose the true relationship between $X$ and $Y$ is:\n",
    "\n",
    "$$\n",
    "Y = f(X) + \\varepsilon\n",
    "$$\n",
    "\n",
    "where $f(X)$ is some unknown function and $\\varepsilon$ is random noise with $E[\\varepsilon] = 0$.\n",
    "\n",
    "Our goal is to estimate $\\hat{f}(X)$ from data. **Different training samples give us different $\\hat{f}$**. So $\\hat{f}$ is itself a random variable!\n",
    "\n",
    "### The Key Question\n",
    "\n",
    "When we make a prediction $\\hat{f}(X)$, how far off will we be on average?\n",
    "\n",
    "The **Mean Squared Error (MSE)** measures this:\n",
    "\n",
    "$$\n",
    "MSE = E\\left[\\left(Y - \\hat{f}(X)\\right)^2\\right]\n",
    "$$\n",
    "\n",
    "It turns out this error can be decomposed into three parts:\n",
    "\n",
    "$$\n",
    "MSE = \\underbrace{\\text{Bias}^2}_{\\text{systematic error}} + \\underbrace{\\text{Variance}}_{\\text{sensitivity to data}} + \\underbrace{\\sigma^2_\\varepsilon}_{\\text{irreducible noise}}\n",
    "$$\n",
    "\n",
    "- **Bias**: How far off is our model *on average*? (Does it systematically miss the target?)\n",
    "- **Variance**: How much does our model *change* with different training data? (Is it stable?)\n",
    "- **Irreducible error**: Random noise we can never predict\n",
    "\n",
    "*Question:* Why is it called \"irreducible error\"?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "No matter how perfect our model becomes, we cannot reduce prediction error to zero. This remaining 'irreducible error' exists because real-world data contains inherent randomness and unmeasured variables that influence outcomes. These unpredictable elements represent the fundamental noise in any system that no model can capture.\n",
    "\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "#### Math detour: bias-variance decomposition\n",
    "\n",
    "To prove the statement above, let's decompose the expected prediction error:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    E\\left[\\left(Y - \\hat{f}(X)\\right)^2\\right] &= E\\left[\\left(f(X) + \\varepsilon - \\hat{f}(X)\\right)^2\\right] \\\\\n",
    "        &= E\\left[\\left(f(X) - \\hat{f}(X)\\right)^2\\right] + 2E\\left[\\varepsilon(f(X) - \\hat{f}(X))\\right] + E\\left[\\varepsilon^2\\right] \\\\\n",
    "        &= E\\left[\\left(f(X) - \\hat{f}(X)\\right)^2\\right] + 2\\underbrace{E[\\varepsilon]}_{=0}E\\left[f(X) - \\hat{f}(X)\\right] + E\\left[\\varepsilon^2\\right] \\\\\n",
    "        &= E\\left[\\left(f(X) - E[\\hat{f}(X)] + E[\\hat{f}(X)] - \\hat{f}(X)\\right)^2\\right] + E\\left[\\varepsilon^2\\right] \\\\\n",
    "        &= E\\left[\\left(f(X) - E[\\hat{f}(X)]\\right)^2\\right] + 2\\underbrace{E\\left[\\left(f(X) - E[\\hat{f}(X)]\\right)\\left(E[\\hat{f}(X)] - \\hat{f}(X)\\right)\\right]}_{=f(x)\\left(E[\\hat{f}(X)] - E[\\hat{f}(X)]\\right) - E[\\hat{f}(X)]^2 + E[\\hat{f}(X)]^2 = 0} + E\\left[\\left(E[\\hat{f}(X)] - \\hat{f}(X)\\right)^2\\right] + E\\left[\\varepsilon^2\\right] \\\\    \n",
    "        &= \\left(\\underbrace{f(X) - E[\\hat{f}(X)]}_{\\text{Bias}} \\right)^2 + \\underbrace{E\\left[\\left(E[\\hat{f}(X)] - \\hat{f}(X)\\right)^2\\right]}_{\\text{Variance}} + \\underbrace{E\\left[\\varepsilon^2\\right]}_{\\text{Irreducible error}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus, MSE = Squared Bias + Variance + Irreducible error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21ddb8",
   "metadata": {},
   "source": [
    "## Seeing bias-variance in action: Linear vs. Quadratic model\n",
    "\n",
    "Let's *see* the trade-off through simulation. We'll compare two models:\n",
    "\n",
    "1. **Simple model** (linear): $\\hat{f}(X) = \\beta_0 + \\beta_1 X$ — *wrong* but stable\n",
    "2. **Complex model** (quadratic): $\\hat{f}(X) = \\beta_0 + \\beta_1 X + \\beta_2 X^2$ — *correct* but more variable\n",
    "\n",
    "### The true relationship\n",
    "\n",
    "We'll generate data from a quadratic function with noise:\n",
    "\n",
    "$$\n",
    "Y = f(X) + \\varepsilon = X^2 - 1.5 X + \\varepsilon, \\quad \\varepsilon \\sim N(0, 1)\n",
    "$$\n",
    "\n",
    "The quadratic model can represent the true relationship perfectly (zero bias), while the linear model cannot (non-zero bias). But which performs better in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d658c60",
   "metadata": {},
   "source": [
    "### Data generating process\n",
    "\n",
    "Below we define:\n",
    "- `trueModel(x)`: the true function $f(X) = X^2 - 1.5X$\n",
    "- `generateData()`: generates samples with random $X$ values and adds noise to create observed $Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8279ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trueModel(x):\n",
    "    y = x**2 - 1.5*x\n",
    "    return y\n",
    "\n",
    "def generateData(prng, sample_size):\n",
    "    x = prng.uniform(0, 1, size=sample_size)\n",
    "    y_true = trueModel(x)\n",
    "    y = y_true + prng.normal(0, 1, size=sample_size)\n",
    "\n",
    "    feature_df = pd.DataFrame({'x': x})\n",
    "    return feature_df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8170f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "features, y = generateData(prng, sample_size=100)\n",
    "\n",
    "plt.plot(features['x'].sort_values(), trueModel(features['x'].sort_values()), label=\"Expected Y\", color='darkred')\n",
    "plt.scatter(features['x'], y, label=\"Observed Y\", alpha=0.5)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Y vs X\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22903435",
   "metadata": {},
   "source": [
    "### Evaluation point\n",
    "\n",
    "To understand bias and variance, we need to evaluate our model at a specific point. We'll use $X_0 = 0.5$ (the middle of our range).\n",
    "\n",
    "Since we know the true model, we know that:\n",
    "\n",
    "$$\n",
    "f(0.5) = 0.5^2 - 1.5 \\times 0.5 = -0.5\n",
    "$$\n",
    "\n",
    "**Important:** In real ML problems, we don't know $f(X)$. That's why we use train-test splits. But in this simulation, knowing the truth lets us directly measure bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1a4738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance at a single point: X=0.5 (y = -0.5)\n",
    "test_data = pd.DataFrame({'x': [0.5]})\n",
    "trueModel(test_data['x'])[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d43f411",
   "metadata": {},
   "source": [
    "### Fitting two competing models\n",
    "\n",
    "Now let's fit both a linear and quadratic model to the *same* data. We'll see how their predictions at $X_0 = 0.5$ differ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b6514",
   "metadata": {},
   "source": [
    "#### Technical detour: pandas Dataframe vs Series\n",
    "\n",
    "If `data` is a pandas Dataframe, then `data['x']` is a pandas Series. Pandas Series are like numpy arrays but with additional functionality.\n",
    "If you would like to pass a Dataframe with a single column, you can use the `data[['x']]` syntax. (Think of it as the standard way of selecting columns from a Dataframe that you would do with  `data[list_of_columns]` syntax, e.g. `data[['x', 'y']]`. For a single column, this just collapses to `data[['x']]`.)\n",
    "If you would like to get a scalar value, you should index it again. To get the first element (which could be the only element), you can use `data['x'][0]`.\n",
    "\n",
    "Be careful with the different types of objects you get back. Some methods expect pandas Dataframes, others might expect scalars, or numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4025a859",
   "metadata": {},
   "source": [
    "#### Linear model\n",
    "\n",
    "We use `Pipeline` as you did in DA3. The usual `sklearn` syntax of definition -> fit -> transform/predict applies for pipelines as well. We will use our sample (`features`) to fit the models, and the middle point (`test_data`) for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b03092f-9846-41f4-907f-ea537ead7c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = Pipeline([(\"lm\", LinearRegression())])\n",
    "simple_model.fit(features, y)\n",
    "simple_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feabb6d",
   "metadata": {},
   "source": [
    "#### Quadratic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73258d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_model = Pipeline([\n",
    "    (\"add-quadratic-term\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"lm\", LinearRegression())\n",
    "])\n",
    "squared_model.fit(features, y)\n",
    "\n",
    "squared_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c0bba",
   "metadata": {},
   "source": [
    "## Monte Carlo simulation: The core experiment\n",
    "\n",
    "One sample gives us one prediction. But to understand **bias** and **variance**, we need to see what happens across *many* different training samples.\n",
    "\n",
    "**The idea:** If we could draw many independent training datasets from the same data generating process, we could:\n",
    "- Estimate **bias** as the average deviation from the truth\n",
    "- Estimate **variance** as how spread out our predictions are\n",
    "\n",
    "This is exactly what a **Monte Carlo simulation** does: we repeatedly simulate the data-generating process and observe the distribution of outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d3855c",
   "metadata": {},
   "source": [
    "### What is Monte Carlo simulation?\n",
    "\n",
    "**Monte Carlo methods** use repeated random sampling to understand the behavior of systems that have randomness. Named after the famous casino, these methods \"roll the dice\" many times to see what typically happens.\n",
    "\n",
    "In our context:\n",
    "1. Generate a new random training dataset\n",
    "2. Fit both models to this dataset\n",
    "3. Record the predictions at $X_0 = 0.5$\n",
    "4. Repeat 1000 times\n",
    "5. Analyze the distribution of predictions\n",
    "\n",
    "This lets us observe bias and variance directly, rather than just deriving them mathematically.\n",
    "\n",
    "**Connection to econometrics:** This is similar to how you might simulate the sampling distribution of an estimator (e.g., showing that OLS is unbiased by simulating many samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385ebda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of Monte Carlo iterations and sample size\n",
    "num_iterations = 1000\n",
    "sample_size = 100\n",
    "\n",
    "# Initialize arrays to store the results\n",
    "predictions = np.empty((num_iterations, 2))\n",
    "\n",
    "# Perform the Monte Carlo simulation\n",
    "for i in range(num_iterations):\n",
    "\n",
    "    features, y = generateData(prng, sample_size=sample_size)\n",
    "    \n",
    "    simple_fit = simple_model.fit(features, y)\n",
    "    squared_fit = squared_model.fit(features, y)\n",
    "    \n",
    "    predictions[i, 0] = simple_model.predict(test_data)[0]\n",
    "    predictions[i, 1] = squared_model.predict(test_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbdeade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the results\n",
    "pd.DataFrame({\n",
    "    'model': ['simple', 'squared'],\n",
    "    'x0': test_data['x'][0],\n",
    "    'bias': np.mean(predictions - trueModel(test_data['x'])[0], axis=0),\n",
    "    'variance': np.var(predictions, axis=0),\n",
    "    'mse': np.mean(np.square(predictions - trueModel(test_data['x'])[0]), axis=0)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ac86c",
   "metadata": {},
   "source": [
    "### Interpreting the results\n",
    "\n",
    "Look at the table above carefully:\n",
    "- **Simple model**: Higher bias (wrong functional form), but lower variance (more stable)\n",
    "- **Quadratic model**: Near-zero bias (correct functional form), but higher variance (more sensitive to data)\n",
    "\n",
    "**The surprise:** Despite being \"wrong,\" the simple model has lower MSE! It wins because its stability (low variance) more than compensates for its systematic error (bias).\n",
    "\n",
    "*Question:* Which of the parameters (`num_iterations`, `sample_size`) should be changed to alter this result?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "The `sample_size` parameter along with the implicit parameter of the variance of the noise term $\\varepsilon$ together determine the **signal-to-noise ratio**. \n",
    "\n",
    "With larger samples, the true patterns become more distinguishable from random noise, which enables the complex model to benefit more from its accurate representation of the true relationship than what it loses on its higher variance. \n",
    "\n",
    "**Try it:** Re-run the simulation with `sample_size=500` and compare the results. The quadratic model should start winning!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392a2319",
   "metadata": {},
   "source": [
    "### Visualizing the trade-off\n",
    "\n",
    "To build intuition, we'll plot:\n",
    "1. The **fitted functions** from many simulations (to see variance visually)\n",
    "2. The **sampling distribution of coefficients** (to see how estimates vary)\n",
    "\n",
    "A high-variance model produces wildly different predictions depending on which training data we happen to get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8be735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the number of Monte Carlo iterations and sample size\n",
    "num_iterations = 100\n",
    "sample_sizes = [100, 1000]\n",
    "\n",
    "# Initialize chart\n",
    "fig, (axs) = plt.subplots(2, 2, sharey=True, sharex=True)\n",
    "\n",
    "beta1_predictions = np.empty((num_iterations, 2, 2))\n",
    "\n",
    "# Perform the Monte Carlo simulation\n",
    "for i in range(num_iterations):\n",
    "\n",
    "    for ids, s in enumerate(sample_sizes):\n",
    "        features, y = generateData(prng, sample_size=s)\n",
    "\n",
    "        simple_model = simple_model.fit(features, y)\n",
    "        squared_model = squared_model.fit(features, y)\n",
    "\n",
    "        simple_prediction = simple_model.predict(features.sort_values(by='x'))  # we need to sort them for the plot\n",
    "        squared_prediction = squared_model.predict(features.sort_values(by='x'))\n",
    "        \n",
    "        beta1_predictions[i, ids, 0] = simple_model['lm'].coef_[0]\n",
    "        beta1_predictions[i, ids, 1] = squared_model['lm'].coef_[0]\n",
    "\n",
    "        axs[ids, 0].plot(features['x'].sort_values(), simple_prediction, color='black', alpha=0.05)\n",
    "        axs[ids, 1].plot(features['x'].sort_values(), squared_prediction, color='black', alpha=0.05)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.plot(features['x'].sort_values(), trueModel(features['x'].sort_values()), color='darkred', linestyle='dashed')\n",
    "    ax.axvline(x=0.5, ymin=0, ymax=1, linestyle=\"dotted\", color=\"black\", linewidth=1)\n",
    "    ax.annotate(r\"$X_0$\", xy=(0.5, ax.get_ylim()[1]), xytext=(0.55, 0.5), fontsize=10)\n",
    "    \n",
    "fig.suptitle(r\"Predicted $\\hat{f}(X)$ vs $f(X)$\")\n",
    "axs[0, 0].set_title(\"Linear (n=100)\")\n",
    "axs[0, 1].set_title(\"Quadratic (n=100)\")\n",
    "axs[1, 0].set_title(\"Linear (n=1000)\")\n",
    "axs[1, 1].set_title(\"Quadratic (n=1000)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2479f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (axs) = plt.subplots(2, 2, sharey=True, sharex=True)\n",
    "for ax1 in range(2):\n",
    "    for ax2 in range (2):\n",
    "        s = sns.kdeplot(beta1_predictions[:, ax1, ax2], ax=axs[ax1, ax2])\n",
    "        s.axvline(x=-1.5, ymin=0, ymax=1, linestyle=\"dashed\", color=\"black\", linewidth=1)\n",
    "\n",
    "fig.suptitle(r\"Sampling distribution of $\\beta_1$\")\n",
    "axs[0, 0].set_title(\"Linear (n=100)\")\n",
    "axs[0, 1].set_title(\"Quadratic (n=100)\")\n",
    "axs[1, 0].set_title(\"Linear (n=1000)\")\n",
    "axs[1, 1].set_title(\"Quadratic (n=1000)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35842a8",
   "metadata": {},
   "source": [
    "**Interpreting the coefficient distributions:**\n",
    "- The dashed line shows the true $\\beta_1 = -1.5$\n",
    "- **Linear model**: The distribution is tight (low variance) but not centered on -1.5 (biased)\n",
    "- **Quadratic model**: The distribution is centered on -1.5 (unbiased) but more spread out (high variance)\n",
    "- **Sample size effect**: With n=1000, both distributions tighten up, and the quadratic model's advantage becomes clearer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19483a73",
   "metadata": {},
   "source": [
    "## Bias-variance in multidimensional problems\n",
    "\n",
    "### New setup: Two predictors\n",
    "\n",
    "Now we have:\n",
    "$$\n",
    "Y = X_1 + X_2 + \\varepsilon, \\quad \\varepsilon \\sim N(0, 2)\n",
    "$$\n",
    "\n",
    "Both predictors are relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a56630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trueModel(x1, x2):\n",
    "    y = x1 + x2\n",
    "    return y\n",
    "\n",
    "def generateData(prng, sample_size):\n",
    "    features = [prng.uniform(0, 1, size=sample_size) for _ in range(3)]\n",
    "    y_true = trueModel(features[0], features[1])\n",
    "    y = y_true + prng.normal(0, 2, size=sample_size) # sigma(epsilon) = 2\n",
    " \n",
    "    feature_df = pd.DataFrame({\n",
    "        'x1': features[0],\n",
    "        'x2': features[1]\n",
    "    })\n",
    "\n",
    "    return feature_df, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c96bde",
   "metadata": {},
   "source": [
    "### Evaluation point\n",
    "\n",
    "We'll evaluate at $(X_1, X_2) = (0, 0)$, where the true value is $f(0, 0) = 0$.\n",
    "\n",
    "This is an **extrapolation point** (at the boundary of our data), which makes predictions more challenging and amplifies the variance problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40515b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance at a single point: X=0\n",
    "test_data = pd.DataFrame({'x1': [0], 'x2': [0]})\n",
    "trueModel(test_data['x1'], test_data['x2'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da13995e",
   "metadata": {},
   "source": [
    "### Balancing between simple and complex: regularization\n",
    "\n",
    "**Regularization** Use a complex model but *constrain* its flexibility to reduce variance.\n",
    "\n",
    "#### From OLS to Lasso\n",
    "\n",
    "You know OLS minimizes squared errors:\n",
    "$$\n",
    "\\min_\\beta \\sum_i (Y_i - X_i'\\beta)^2\n",
    "$$\n",
    "\n",
    "**Lasso** (Least Absolute Shrinkage and Selection Operator) adds a penalty on coefficient size:\n",
    "$$\n",
    "\\min_\\beta \\sum_i (Y_i - X_i'\\beta)^2 + \\alpha \\sum_j |\\beta_j|\n",
    "$$\n",
    "\n",
    "The penalty term $\\alpha \\sum_j |\\beta_j|$ does two things:\n",
    "1. **Shrinks** coefficients toward zero (reduces variance)\n",
    "2. **Selects** features by setting some coefficients exactly to zero (sparsity)\n",
    "\n",
    "The penalty parameter $\\alpha$ controls the trade-off:\n",
    "  - $\\alpha = 0$: identical to OLS (no penalty)\n",
    "  - $\\alpha \\to \\infty$: all coefficients shrunk to zero\n",
    "  \n",
    "**Key insight:** By accepting some bias (shrinking coefficients), we can substantially reduce variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af35326",
   "metadata": {},
   "source": [
    "### Fitting two competing models: OLS and Lasso\n",
    "\n",
    "**Exercise:** Before looking at the code below, think about what you expect:\n",
    "- Both models predict $\\hat{f}(0, 0)$ when the true value is $f(0, 0) = 0$\n",
    "- OLS has no penalty, Lasso has default $\\alpha = 1.0$ (quite strong shrinkage)\n",
    "- With only 20 observations and noise std = 2, how stable will each model be?\n",
    "\n",
    "The code below fits both models and shows their predictions. Notice how the Lasso coefficients get shrunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc821012",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 20\n",
    "\n",
    "X, y = generateData(prng, sample_size)\n",
    "\n",
    "lm = LinearRegression().fit(X, y)\n",
    "lasso = Lasso(random_state=prng).fit(X, y)\n",
    "\n",
    "print(\"Linear model prediction: \", lm.predict(test_data)[0])\n",
    "print(\"Lasso prediction: \", lasso.predict(test_data)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edff35ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e360ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "?Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8a3117",
   "metadata": {},
   "source": [
    "### Monte Carlo simulation: Lasso vs. OLS\n",
    "\n",
    "Now we'll run another Monte Carlo simulation to compare:\n",
    "- 1 OLS model (no penalty)\n",
    "- 20 Lasso models with different penalty parameters $\\alpha$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5bb7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo simulation\n",
    "\n",
    "n_iterations = 1000\n",
    "alphas_to_try = np.linspace(0.01, 0.5, num=20)\n",
    "\n",
    "lm_predictions = np.empty(n_iterations)\n",
    "lasso_predictions = np.empty((n_iterations, len(alphas_to_try)))\n",
    "lasso_n_coeffs = np.empty((n_iterations, len(alphas_to_try)))\n",
    "\n",
    "# Perform the Monte Carlo simulation\n",
    "for i in range(n_iterations):\n",
    "\n",
    "    X, y = generateData(prng, sample_size)\n",
    "\n",
    "    lm_model = lm.fit(X, y)\n",
    "    lm_predictions[i] = lm_model.predict(test_data)[0]\n",
    "\n",
    "    for ida, a in enumerate(alphas_to_try):\n",
    "        lasso_model = Lasso(alpha=a, random_state=prng).fit(X, y)\n",
    "        lasso_predictions[i, ida] = lasso_model.predict(test_data)[0]\n",
    "        lasso_n_coeffs[i, ida] = np.count_nonzero(lasso_model.coef_)  # we would like to count the non-zero coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314af9ef",
   "metadata": {},
   "source": [
    "### Combining results\n",
    "\n",
    "Linear Regression is a special case of Lasso with $\\alpha = 0$. Let's concatenate the predictions for a unified analysis.\n",
    "\n",
    "(*Note: `lm_predictions` is 1D, so we reshape it to 2D with `reshape(-1, 1)` before concatenation.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ec6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.concatenate([lm_predictions.reshape(-1, 1), lasso_predictions], axis=1)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd0780",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.concatenate([[0], alphas_to_try])\n",
    "biases = np.mean(predictions - trueModel(test_data['x1'], test_data['x2'])[0], axis=0)\n",
    "variances = np.var(predictions, axis=0)\n",
    "mses = np.mean(np.square(predictions - trueModel(test_data['x1'], test_data['x2'])[0]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd2ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, biases**2, label='Bias^2')\n",
    "plt.plot(alphas, variances, label='Variance')\n",
    "plt.plot(alphas, mses, label='MSE')\n",
    "plt.xlabel('Penalty parameter')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b86d00",
   "metadata": {},
   "source": [
    "### The bias-variance trade-off curve\n",
    "\n",
    "Look at the plot above:\n",
    "- **At $\\alpha = 0$ (OLS)**: Zero bias but highest variance\n",
    "- **As $\\alpha$ increases**: Bias increases (coefficients shrink away from truth), but variance decreases\n",
    "- **Optimal $\\alpha$**: MSE is minimized somewhere in between!\n",
    "\n",
    "**Key insight:** A penalized regression with the right $\\alpha$ outperforms the \"true\" unpenalized model! This is the bias-variance trade-off in action: we accept some bias to substantially reduce variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b4c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_num_coeffs = np.concatenate([[2], np.mean(lasso_n_coeffs, axis=0)])\n",
    "plt.plot(alphas, avg_num_coeffs, label='Avg number of non-zero coefficients')\n",
    "plt.xlabel('Penalty parameter')\n",
    "plt.title('Variable selection in Lasso by the penalty parameter')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6ec5c",
   "metadata": {},
   "source": [
    "## From simulation to practice: Cross-validation\n",
    "\n",
    "In our simulation, we knew the true model and could compute bias and MSE directly. But in real ML:\n",
    "- We don't know $f(X)$\n",
    "- We can't generate unlimited data\n",
    "- We need to estimate the optimal $\\alpha$ from our single dataset\n",
    "\n",
    "**Solution:** **Cross-validation**\n",
    "\n",
    "The idea:\n",
    "1. Split data into K folds\n",
    "2. For each candidate $\\alpha$, train on K-1 folds and evaluate on the held-out fold\n",
    "3. Average performance across folds\n",
    "4. Choose $\\alpha$ with best cross-validated performance\n",
    "\n",
    "This estimates MSE without knowing the true function, and will be a key technique throughout this course.\n",
    "\n",
    "---\n",
    "\n",
    "## Key takeaways\n",
    "\n",
    "1. **Bias-variance trade-off**: More complex models have lower bias but higher variance\n",
    "2. **Simple can beat complex**: When sample size is small or noise is high, stable (biased) models often outperform accurate (high-variance) ones\n",
    "3. **Regularization helps**: Techniques like Lasso deliberately introduce bias to reduce variance\n",
    "4. **Optimal complexity depends on data**: More data → can afford more complexity\n",
    "5. **Cross-validation**: In practice, use CV to find the right balance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
